{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%run auxTensor.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print('GPU found')\n",
    "else:\n",
    "    print(\"No GPU found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generating TENSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a new dataframe without containing the last release...\n",
      "... DONE!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>CountClassCoupled</th>\n",
       "      <th>SumCyclomatic</th>\n",
       "      <th>MaxInheritanceTree</th>\n",
       "      <th>PercentLackOfCohesion</th>\n",
       "      <th>CountLineCode</th>\n",
       "      <th>CountClassDerived</th>\n",
       "      <th>CountDeclMethodAll</th>\n",
       "      <th>CountDeclMethod</th>\n",
       "      <th>will_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1693</th>\n",
       "      <td>12330535103010800000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1694</th>\n",
       "      <td>17718834335385200000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1695</th>\n",
       "      <td>10086132970827500000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1696</th>\n",
       "      <td>352269180384505000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1697</th>\n",
       "      <td>12806131860581700000</td>\n",
       "      <td>8</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>278</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1698</th>\n",
       "      <td>11661319254065500000</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>13256513995510200000</td>\n",
       "      <td>17</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0.54</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700</th>\n",
       "      <td>5120380230823330000</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.56</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1701</th>\n",
       "      <td>8342725214105680000</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1702</th>\n",
       "      <td>4745566235454730000</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1703</th>\n",
       "      <td>9203413018515720000</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704</th>\n",
       "      <td>5191702074174610000</td>\n",
       "      <td>17</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0.35</td>\n",
       "      <td>107</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1705</th>\n",
       "      <td>1020408308067090000</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1706</th>\n",
       "      <td>12532200831695100000</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1707</th>\n",
       "      <td>1290048571602050000</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1708</th>\n",
       "      <td>2838776917771780000</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1709</th>\n",
       "      <td>1763159113737380000</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710</th>\n",
       "      <td>6394391764156730000</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0.61</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1711</th>\n",
       "      <td>8449865229693840000</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1712</th>\n",
       "      <td>9711438674336600000</td>\n",
       "      <td>18</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>0.29</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1713</th>\n",
       "      <td>1607237642681600000</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1714</th>\n",
       "      <td>11629025425017000000</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1715</th>\n",
       "      <td>11297746963483500000</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1716</th>\n",
       "      <td>13103624532371300000</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1717</th>\n",
       "      <td>13686089118630300000</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1718</th>\n",
       "      <td>3741725236513220000</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1719</th>\n",
       "      <td>7976621627129360000</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1720</th>\n",
       "      <td>11614657698239200000</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1721</th>\n",
       "      <td>5365356906327730000</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1722</th>\n",
       "      <td>14776773266497500000</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11546</th>\n",
       "      <td>10938371596722700000</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11547</th>\n",
       "      <td>15844046047036100000</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.67</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11548</th>\n",
       "      <td>529866715401650000</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11549</th>\n",
       "      <td>13516211893773400000</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11550</th>\n",
       "      <td>1621655042592430000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11551</th>\n",
       "      <td>12352196163979000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11552</th>\n",
       "      <td>3156466011518350000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11553</th>\n",
       "      <td>14637744378044200000</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11554</th>\n",
       "      <td>4337086210505710000</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11555</th>\n",
       "      <td>1415626860735340000</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11556</th>\n",
       "      <td>9377077900475950000</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11557</th>\n",
       "      <td>233877332338132000</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11558</th>\n",
       "      <td>6578198509721350000</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11559</th>\n",
       "      <td>7374181385141520000</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11560</th>\n",
       "      <td>7655830626322890000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11561</th>\n",
       "      <td>4964520095947590000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11562</th>\n",
       "      <td>13007524891799400000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11563</th>\n",
       "      <td>3174773910002310000</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11564</th>\n",
       "      <td>12975742097278000000</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11565</th>\n",
       "      <td>3556467704862370000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11566</th>\n",
       "      <td>6655937819891080000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11567</th>\n",
       "      <td>4773483168370670000</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11568</th>\n",
       "      <td>12151893705439500000</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11569</th>\n",
       "      <td>11545482127785900000</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11570</th>\n",
       "      <td>3216044138335310000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11571</th>\n",
       "      <td>7382564503316640000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11572</th>\n",
       "      <td>8679182117657290000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11573</th>\n",
       "      <td>1573996298580430000</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11574</th>\n",
       "      <td>17633672838296100000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11575</th>\n",
       "      <td>15626811517434900000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9883 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Path  CountClassCoupled  SumCyclomatic  \\\n",
       "1693   12330535103010800000                  3              1   \n",
       "1694   17718834335385200000                  3              1   \n",
       "1695   10086132970827500000                  3              1   \n",
       "1696     352269180384505000                  2              1   \n",
       "1697   12806131860581700000                  8             73   \n",
       "1698   11661319254065500000                  9              4   \n",
       "1699   13256513995510200000                 17             26   \n",
       "1700    5120380230823330000                 12             12   \n",
       "1701    8342725214105680000                 13              4   \n",
       "1702    4745566235454730000                 13              9   \n",
       "1703    9203413018515720000                  8              6   \n",
       "1704    5191702074174610000                 17             43   \n",
       "1705    1020408308067090000                 11             14   \n",
       "1706   12532200831695100000                 13              4   \n",
       "1707    1290048571602050000                 13              2   \n",
       "1708    2838776917771780000                 13              2   \n",
       "1709    1763159113737380000                 14              2   \n",
       "1710    6394391764156730000                 12             16   \n",
       "1711    8449865229693840000                 14             22   \n",
       "1712    9711438674336600000                 18             49   \n",
       "1713    1607237642681600000                 13              2   \n",
       "1714   11629025425017000000                 13              2   \n",
       "1715   11297746963483500000                 11              4   \n",
       "1716   13103624532371300000                  5              2   \n",
       "1717   13686089118630300000                  8              4   \n",
       "1718    3741725236513220000                 13             13   \n",
       "1719    7976621627129360000                 22              2   \n",
       "1720   11614657698239200000                 14              4   \n",
       "1721    5365356906327730000                  7              6   \n",
       "1722   14776773266497500000                 10              8   \n",
       "...                     ...                ...            ...   \n",
       "11546  10938371596722700000                  0              8   \n",
       "11547  15844046047036100000                  0             14   \n",
       "11548    529866715401650000                  0              9   \n",
       "11549  13516211893773400000                  0              6   \n",
       "11550   1621655042592430000                  0              1   \n",
       "11551  12352196163979000000                  0              1   \n",
       "11552   3156466011518350000                  0              1   \n",
       "11553  14637744378044200000                  0             12   \n",
       "11554   4337086210505710000                  0             16   \n",
       "11555   1415626860735340000                  6             14   \n",
       "11556   9377077900475950000                  0             10   \n",
       "11557    233877332338132000                  0             22   \n",
       "11558   6578198509721350000                  0             11   \n",
       "11559   7374181385141520000                  0             11   \n",
       "11560   7655830626322890000                  0              0   \n",
       "11561   4964520095947590000                  0              3   \n",
       "11562  13007524891799400000                  0              1   \n",
       "11563   3174773910002310000                  0              7   \n",
       "11564  12975742097278000000                  0              4   \n",
       "11565   3556467704862370000                  0              1   \n",
       "11566   6655937819891080000                  0              3   \n",
       "11567   4773483168370670000                  0              6   \n",
       "11568  12151893705439500000                  0              4   \n",
       "11569  11545482127785900000                  0              9   \n",
       "11570   3216044138335310000                  0              5   \n",
       "11571   7382564503316640000                  0              1   \n",
       "11572   8679182117657290000                  0              2   \n",
       "11573   1573996298580430000                  0              9   \n",
       "11574  17633672838296100000                  0              2   \n",
       "11575  15626811517434900000                  0              2   \n",
       "\n",
       "       MaxInheritanceTree  PercentLackOfCohesion  CountLineCode  \\\n",
       "1693                    0                   0.00             30   \n",
       "1694                    0                   0.00             27   \n",
       "1695                    0                   0.00             30   \n",
       "1696                    0                   0.00             14   \n",
       "1697                    0                   0.00            278   \n",
       "1698                    0                   0.00             21   \n",
       "1699                    0                   0.54             51   \n",
       "1700                    0                   0.56             39   \n",
       "1701                    0                   0.50             11   \n",
       "1702                    0                   0.60             20   \n",
       "1703                    0                   0.00             18   \n",
       "1704                    0                   0.35            107   \n",
       "1705                    0                   0.50             34   \n",
       "1706                    0                   0.50             11   \n",
       "1707                    0                   0.00              7   \n",
       "1708                    0                   0.00              7   \n",
       "1709                    0                   0.00              9   \n",
       "1710                    0                   0.61             29   \n",
       "1711                    0                   0.33             46   \n",
       "1712                    0                   0.29             70   \n",
       "1713                    0                   0.00              7   \n",
       "1714                    0                   0.00              7   \n",
       "1715                    0                   0.00              9   \n",
       "1716                    0                   0.50              6   \n",
       "1717                    0                   0.00             10   \n",
       "1718                    0                   0.33             41   \n",
       "1719                    0                   0.00             12   \n",
       "1720                    0                   0.00             11   \n",
       "1721                    0                   0.00             10   \n",
       "1722                    0                   0.00             30   \n",
       "...                   ...                    ...            ...   \n",
       "11546                   0                   0.00             35   \n",
       "11547                   0                   0.67             17   \n",
       "11548                   0                   0.00              9   \n",
       "11549                   0                   0.00              0   \n",
       "11550                   0                   0.00              1   \n",
       "11551                   0                   0.00              1   \n",
       "11552                   0                   0.00              1   \n",
       "11553                   0                   0.00             12   \n",
       "11554                   0                   0.00             24   \n",
       "11555                   2                   0.00             16   \n",
       "11556                   0                   0.00             10   \n",
       "11557                   0                   0.00             33   \n",
       "11558                   0                   0.00             11   \n",
       "11559                   0                   0.00             11   \n",
       "11560                   0                   0.00              0   \n",
       "11561                   0                   0.00              3   \n",
       "11562                   0                   0.00              4   \n",
       "11563                   0                   0.00              7   \n",
       "11564                   2                   0.00              4   \n",
       "11565                   3                   0.00              1   \n",
       "11566                   0                   0.00              3   \n",
       "11567                   0                   0.00              9   \n",
       "11568                   0                   0.00              5   \n",
       "11569                   0                   0.00             11   \n",
       "11570                   0                   0.00              7   \n",
       "11571                   3                   0.00              1   \n",
       "11572                   0                   0.00              2   \n",
       "11573                   0                   0.00              9   \n",
       "11574                   0                   0.00              2   \n",
       "11575                   0                   0.00              2   \n",
       "\n",
       "       CountClassDerived  CountDeclMethodAll  CountDeclMethod  will_change  \n",
       "1693                   0                   5                1            0  \n",
       "1694                   0                   5                1            0  \n",
       "1695                   0                   5                1            0  \n",
       "1696                   0                   2                1            0  \n",
       "1697                   0                  46               21            0  \n",
       "1698                   0                  11                1            0  \n",
       "1699                   0                  25               12            0  \n",
       "1700                   0                  15                8            0  \n",
       "1701                   0                   6                3            0  \n",
       "1702                   0                  10                4            0  \n",
       "1703                   0                   7                1            0  \n",
       "1704                   0                  27               16            0  \n",
       "1705                   0                  15                5            0  \n",
       "1706                   0                   6                3            0  \n",
       "1707                   0                   6                1            0  \n",
       "1708                   0                   6                1            0  \n",
       "1709                   0                   7                1            0  \n",
       "1710                   0                  22                5            0  \n",
       "1711                   0                  27                5            0  \n",
       "1712                   0                  32                6            0  \n",
       "1713                   0                   6                1            0  \n",
       "1714                   0                   6                1            0  \n",
       "1715                   0                  18                1            0  \n",
       "1716                   0                   1                1            0  \n",
       "1717                   0                  10                1            0  \n",
       "1718                   0                  21                2            0  \n",
       "1719                   0                  17                1            0  \n",
       "1720                   0                   9                1            0  \n",
       "1721                   0                  12                1            0  \n",
       "1722                   0                  15                1            0  \n",
       "...                  ...                 ...              ...          ...  \n",
       "11546                  0                   0                3            0  \n",
       "11547                  0                   0                1            0  \n",
       "11548                  0                   0                0            0  \n",
       "11549                  0                   0                0            0  \n",
       "11550                  0                   0                0            0  \n",
       "11551                  0                   0                0            0  \n",
       "11552                  0                   0                0            0  \n",
       "11553                  1                   0                1            0  \n",
       "11554                  2                   0                2            0  \n",
       "11555                  1                   0                1            0  \n",
       "11556                  0                   0                1            0  \n",
       "11557                  0                   0                2            0  \n",
       "11558                  0                   0                0            0  \n",
       "11559                  0                   0                0            0  \n",
       "11560                  0                   0                0            0  \n",
       "11561                  0                   0                0            0  \n",
       "11562                  0                   0                0            0  \n",
       "11563                  0                   0                0            0  \n",
       "11564                  0                   0                0            0  \n",
       "11565                  0                   0                0            0  \n",
       "11566                  0                   0                0            0  \n",
       "11567                  0                   0                0            0  \n",
       "11568                  0                   0                0            0  \n",
       "11569                  0                   0                0            0  \n",
       "11570                  0                   0                0            0  \n",
       "11571                  0                   0                0            0  \n",
       "11572                  0                   0                0            0  \n",
       "11573                  0                   0                0            0  \n",
       "11574                  0                   0                0            0  \n",
       "11575                  0                   0                1            0  \n",
       "\n",
       "[9883 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_releases_df = pd.read_csv('raw_data.csv')\n",
    "\n",
    "#all_releases_df.head()\n",
    "\n",
    "print(\"Generating a new dataframe without containing the last release...\")\n",
    "df = all_releases_df[all_releases_df['releaseID'] != all_releases_df['releaseID'].max()]\n",
    "print(\"... DONE!\")\n",
    "\n",
    "df.drop(columns=['class_frequency', 'number_of_changes', 'change_probability', 'instanceID', 'releaseID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 2\n",
      "generating tensor...\n",
      "...DONE!\n",
      "tensor dimensions:\n",
      "tensor input X: (6699, 8, 2)\n",
      "tensor input y: (6699, 2)\n",
      "proportion of y labels: (array([0, 1]), array([ 278, 6421]))\n",
      "Splitting dataset into Train and Test sets...\n",
      "Tensor X train: (4689, 2, 8)\n",
      "Tensor y train: (4689, 2)\n",
      "Tensor X test: (2010, 2, 8)\n",
      "Tensor y test: (2010, 2)\n",
      "computing weights...\n",
      "... DONE!\n",
      "setting stratified k-fold...\n",
      "number of k: 5\n",
      "... DONE!\n",
      "Executing algorithm...\n",
      "number of epochs: 20\n",
      "number of batch: 512\n",
      "Training on fold 1/5...\n",
      "model dim:  (2, 8) 2\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_1 (GRU)                  (None, 2, 256)            203520    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 2, 128)            147840    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 64)                37056     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 388,546\n",
      "Trainable params: 388,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From <ipython-input-1-fb283a009ca9>:2: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, use\n",
      "    tf.py_function, which takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    \n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 3751 samples, validate on 938 samples\n",
      "Epoch 1/20\n",
      "3751/3751 [==============================] - 1s 246us/step - loss: 0.0473 - acc: 0.6870 - auc: 0.6699 - f1: 0.6878 - val_loss: 0.0433 - val_acc: 0.7441 - val_auc: 0.7955 - val_f1: 0.7441\n",
      "Epoch 2/20\n",
      "3751/3751 [==============================] - 0s 40us/step - loss: 0.0456 - acc: 0.7977 - auc: 0.7402 - f1: 0.7977 - val_loss: 0.0423 - val_acc: 0.7441 - val_auc: 0.8037 - val_f1: 0.7441\n",
      "Epoch 3/20\n",
      "3751/3751 [==============================] - 0s 40us/step - loss: 0.0454 - acc: 0.6468 - auc: 0.7530 - f1: 0.6468 - val_loss: 0.0412 - val_acc: 0.7772 - val_auc: 0.8110 - val_f1: 0.7772\n",
      "Epoch 4/20\n",
      "3751/3751 [==============================] - 0s 39us/step - loss: 0.0440 - acc: 0.8051 - auc: 0.7623 - f1: 0.8051 - val_loss: 0.0401 - val_acc: 0.7655 - val_auc: 0.8215 - val_f1: 0.7655\n",
      "Epoch 5/20\n",
      "3751/3751 [==============================] - 0s 39us/step - loss: 0.0427 - acc: 0.7187 - auc: 0.7801 - f1: 0.7187 - val_loss: 0.0396 - val_acc: 0.7857 - val_auc: 0.8179 - val_f1: 0.7857\n",
      "Epoch 6/20\n",
      "3751/3751 [==============================] - 0s 39us/step - loss: 0.0429 - acc: 0.8003 - auc: 0.7694 - f1: 0.8003 - val_loss: 0.0395 - val_acc: 0.7900 - val_auc: 0.8230 - val_f1: 0.7900\n",
      "Epoch 7/20\n",
      "3751/3751 [==============================] - 0s 39us/step - loss: 0.0419 - acc: 0.7705 - auc: 0.7811 - f1: 0.7705 - val_loss: 0.0395 - val_acc: 0.7761 - val_auc: 0.8271 - val_f1: 0.7761\n",
      "Epoch 8/20\n",
      "3751/3751 [==============================] - 0s 38us/step - loss: 0.0422 - acc: 0.7945 - auc: 0.7803 - f1: 0.7945 - val_loss: 0.0394 - val_acc: 0.7367 - val_auc: 0.8300 - val_f1: 0.7367\n",
      "Epoch 9/20\n",
      "3751/3751 [==============================] - 0s 39us/step - loss: 0.0411 - acc: 0.7201 - auc: 0.7942 - f1: 0.7201 - val_loss: 0.0384 - val_acc: 0.8166 - val_auc: 0.8315 - val_f1: 0.8166\n",
      "Epoch 10/20\n",
      "3751/3751 [==============================] - 0s 39us/step - loss: 0.0424 - acc: 0.7987 - auc: 0.7851 - f1: 0.7987 - val_loss: 0.0386 - val_acc: 0.7377 - val_auc: 0.8420 - val_f1: 0.7377\n",
      "Epoch 11/20\n",
      "3751/3751 [==============================] - 0s 39us/step - loss: 0.0407 - acc: 0.7675 - auc: 0.8010 - f1: 0.7675 - val_loss: 0.0395 - val_acc: 0.8262 - val_auc: 0.8185 - val_f1: 0.8262\n",
      "Epoch 12/20\n",
      "3751/3751 [==============================] - 0s 40us/step - loss: 0.0415 - acc: 0.7395 - auc: 0.7870 - f1: 0.7395 - val_loss: 0.0390 - val_acc: 0.7356 - val_auc: 0.8323 - val_f1: 0.7356\n",
      "Epoch 13/20\n",
      "3751/3751 [==============================] - 0s 40us/step - loss: 0.0410 - acc: 0.7889 - auc: 0.7938 - f1: 0.7889 - val_loss: 0.0381 - val_acc: 0.7463 - val_auc: 0.8448 - val_f1: 0.7463\n",
      "Epoch 14/20\n",
      "3751/3751 [==============================] - 0s 40us/step - loss: 0.0407 - acc: 0.7718 - auc: 0.7949 - f1: 0.7718 - val_loss: 0.0380 - val_acc: 0.7761 - val_auc: 0.8375 - val_f1: 0.7761\n",
      "Epoch 15/20\n",
      "3751/3751 [==============================] - 0s 39us/step - loss: 0.0406 - acc: 0.6958 - auc: 0.8146 - f1: 0.6958 - val_loss: 0.0369 - val_acc: 0.8539 - val_auc: 0.8536 - val_f1: 0.8539\n",
      "Epoch 16/20\n",
      "3751/3751 [==============================] - 0s 39us/step - loss: 0.0398 - acc: 0.7566 - auc: 0.8247 - f1: 0.7566 - val_loss: 0.0377 - val_acc: 0.6930 - val_auc: 0.8458 - val_f1: 0.6930\n",
      "Epoch 17/20\n",
      "3751/3751 [==============================] - 0s 40us/step - loss: 0.0389 - acc: 0.7822 - auc: 0.8300 - f1: 0.7822 - val_loss: 0.0380 - val_acc: 0.8753 - val_auc: 0.8393 - val_f1: 0.8753\n",
      "Epoch 18/20\n",
      "3751/3751 [==============================] - 0s 40us/step - loss: 0.0397 - acc: 0.7985 - auc: 0.8160 - f1: 0.7985 - val_loss: 0.0397 - val_acc: 0.5938 - val_auc: 0.8450 - val_f1: 0.5938\n",
      "Epoch 19/20\n",
      "3751/3751 [==============================] - 0s 40us/step - loss: 0.0390 - acc: 0.7395 - auc: 0.8205 - f1: 0.7395 - val_loss: 0.0380 - val_acc: 0.8934 - val_auc: 0.8488 - val_f1: 0.8934\n",
      "Epoch 20/20\n",
      "3751/3751 [==============================] - 0s 40us/step - loss: 0.0400 - acc: 0.7251 - auc: 0.8236 - f1: 0.7251 - val_loss: 0.0376 - val_acc: 0.7026 - val_auc: 0.8449 - val_f1: 0.7026\n",
      "[[ 28   9]\n",
      " [270 631]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.09      0.76      0.17        37\n",
      "           1       0.99      0.70      0.82       901\n",
      "\n",
      "    accuracy                           0.70       938\n",
      "   macro avg       0.54      0.73      0.49       938\n",
      "weighted avg       0.95      0.70      0.79       938\n",
      "\n",
      "Training on fold 2/5...\n",
      "model dim:  (2, 8) 2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_4 (GRU)                  (None, 2, 256)            203520    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (None, 2, 128)            147840    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "gru_6 (GRU)                  (None, 64)                37056     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 388,546\n",
      "Trainable params: 388,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3751 samples, validate on 938 samples\n",
      "Epoch 1/20\n",
      "3751/3751 [==============================] - 1s 289us/step - loss: 0.0475 - acc: 0.6046 - auc: 0.6926 - f1: 0.6057 - val_loss: 0.0549 - val_acc: 0.8507 - val_auc: 0.6352 - val_f1: 0.8507\n",
      "Epoch 2/20\n",
      "3751/3751 [==============================] - 0s 38us/step - loss: 0.0425 - acc: 0.7979 - auc: 0.7871 - f1: 0.7979 - val_loss: 0.0507 - val_acc: 0.7036 - val_auc: 0.6903 - val_f1: 0.7036\n",
      "Epoch 3/20\n",
      "3751/3751 [==============================] - 0s 39us/step - loss: 0.0424 - acc: 0.7638 - auc: 0.7826 - f1: 0.7638 - val_loss: 0.0504 - val_acc: 0.7878 - val_auc: 0.6779 - val_f1: 0.7878\n",
      "Epoch 4/20\n",
      "3751/3751 [==============================] - 0s 39us/step - loss: 0.0417 - acc: 0.7942 - auc: 0.7832 - f1: 0.7942 - val_loss: 0.0496 - val_acc: 0.7431 - val_auc: 0.6975 - val_f1: 0.7431\n",
      "Epoch 5/20\n",
      "3751/3751 [==============================] - 0s 40us/step - loss: 0.0407 - acc: 0.7662 - auc: 0.8104 - f1: 0.7662 - val_loss: 0.0502 - val_acc: 0.7719 - val_auc: 0.6855 - val_f1: 0.7719\n",
      "Epoch 6/20\n",
      "3751/3751 [==============================] - 0s 40us/step - loss: 0.0411 - acc: 0.8230 - auc: 0.8008 - f1: 0.8230 - val_loss: 0.0484 - val_acc: 0.7122 - val_auc: 0.7039 - val_f1: 0.7122\n",
      "Epoch 7/20\n",
      "3751/3751 [==============================] - 0s 43us/step - loss: 0.0414 - acc: 0.6873 - auc: 0.8117 - f1: 0.6873 - val_loss: 0.0497 - val_acc: 0.8134 - val_auc: 0.6859 - val_f1: 0.8134\n",
      "Epoch 8/20\n",
      "3751/3751 [==============================] - 0s 41us/step - loss: 0.0405 - acc: 0.8486 - auc: 0.8114 - f1: 0.8486 - val_loss: 0.0497 - val_acc: 0.7761 - val_auc: 0.6952 - val_f1: 0.7761\n",
      "Epoch 9/20\n",
      "3751/3751 [==============================] - 0s 40us/step - loss: 0.0400 - acc: 0.7798 - auc: 0.8126 - f1: 0.7798 - val_loss: 0.0489 - val_acc: 0.7889 - val_auc: 0.7111 - val_f1: 0.7889\n",
      "Epoch 10/20\n",
      "3751/3751 [==============================] - 0s 40us/step - loss: 0.0386 - acc: 0.8011 - auc: 0.8245 - f1: 0.8011 - val_loss: 0.0481 - val_acc: 0.8081 - val_auc: 0.7085 - val_f1: 0.8081\n",
      "Epoch 11/20\n",
      "3751/3751 [==============================] - 0s 41us/step - loss: 0.0381 - acc: 0.8272 - auc: 0.8260 - f1: 0.8272 - val_loss: 0.0486 - val_acc: 0.7910 - val_auc: 0.7130 - val_f1: 0.7910\n",
      "Epoch 12/20\n",
      "3751/3751 [==============================] - 0s 41us/step - loss: 0.0388 - acc: 0.8014 - auc: 0.8173 - f1: 0.8014 - val_loss: 0.0489 - val_acc: 0.8273 - val_auc: 0.7042 - val_f1: 0.8273\n",
      "Epoch 13/20\n",
      "3751/3751 [==============================] - 0s 44us/step - loss: 0.0376 - acc: 0.8408 - auc: 0.8278 - f1: 0.8408 - val_loss: 0.0470 - val_acc: 0.8060 - val_auc: 0.7419 - val_f1: 0.8060\n",
      "Epoch 14/20\n",
      "3751/3751 [==============================] - 0s 41us/step - loss: 0.0379 - acc: 0.7849 - auc: 0.8376 - f1: 0.7849 - val_loss: 0.0485 - val_acc: 0.8443 - val_auc: 0.7246 - val_f1: 0.8443\n",
      "Epoch 15/20\n",
      "3751/3751 [==============================] - 0s 40us/step - loss: 0.0378 - acc: 0.7942 - auc: 0.8278 - f1: 0.7942 - val_loss: 0.0466 - val_acc: 0.7751 - val_auc: 0.7236 - val_f1: 0.7751\n",
      "Epoch 16/20\n",
      "3751/3751 [==============================] - 0s 44us/step - loss: 0.0385 - acc: 0.8049 - auc: 0.8258 - f1: 0.8049 - val_loss: 0.0489 - val_acc: 0.8198 - val_auc: 0.7349 - val_f1: 0.8198\n",
      "Epoch 17/20\n",
      "3751/3751 [==============================] - 0s 42us/step - loss: 0.0367 - acc: 0.8163 - auc: 0.8422 - f1: 0.8163 - val_loss: 0.0475 - val_acc: 0.8284 - val_auc: 0.7229 - val_f1: 0.8284\n",
      "Epoch 18/20\n",
      "3751/3751 [==============================] - 0s 41us/step - loss: 0.0365 - acc: 0.7929 - auc: 0.8453 - f1: 0.7929 - val_loss: 0.0476 - val_acc: 0.8326 - val_auc: 0.7180 - val_f1: 0.8326\n",
      "Epoch 19/20\n",
      "3751/3751 [==============================] - 0s 41us/step - loss: 0.0360 - acc: 0.8155 - auc: 0.8527 - f1: 0.8155 - val_loss: 0.0509 - val_acc: 0.8166 - val_auc: 0.7192 - val_f1: 0.8166\n",
      "Epoch 20/20\n",
      "3751/3751 [==============================] - 0s 41us/step - loss: 0.0371 - acc: 0.8227 - auc: 0.8441 - f1: 0.8227 - val_loss: 0.0511 - val_acc: 0.8539 - val_auc: 0.7207 - val_f1: 0.8539\n",
      "[[ 14  23]\n",
      " [114 787]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.11      0.38      0.17        37\n",
      "           1       0.97      0.87      0.92       901\n",
      "\n",
      "    accuracy                           0.85       938\n",
      "   macro avg       0.54      0.63      0.54       938\n",
      "weighted avg       0.94      0.85      0.89       938\n",
      "\n",
      "Training on fold 3/5...\n",
      "model dim:  (2, 8) 2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_7 (GRU)                  (None, 2, 256)            203520    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "gru_8 (GRU)                  (None, 2, 128)            147840    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "gru_9 (GRU)                  (None, 64)                37056     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 388,546\n",
      "Trainable params: 388,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3751 samples, validate on 938 samples\n",
      "Epoch 1/20\n",
      "3751/3751 [==============================] - 1s 341us/step - loss: 0.0496 - acc: 0.7097 - auc: 0.6491 - f1: 0.7121 - val_loss: 0.0470 - val_acc: 0.7367 - val_auc: 0.7133 - val_f1: 0.7367\n",
      "Epoch 2/20\n",
      "3751/3751 [==============================] - 0s 38us/step - loss: 0.0432 - acc: 0.7505 - auc: 0.7715 - f1: 0.7505 - val_loss: 0.0477 - val_acc: 0.8401 - val_auc: 0.6986 - val_f1: 0.8401\n",
      "Epoch 3/20\n",
      "3751/3751 [==============================] - 0s 39us/step - loss: 0.0443 - acc: 0.8168 - auc: 0.7467 - f1: 0.8168 - val_loss: 0.0464 - val_acc: 0.7495 - val_auc: 0.7221 - val_f1: 0.7495\n",
      "Epoch 4/20\n",
      "3751/3751 [==============================] - 0s 42us/step - loss: 0.0428 - acc: 0.6955 - auc: 0.7873 - f1: 0.6955 - val_loss: 0.0463 - val_acc: 0.7825 - val_auc: 0.7188 - val_f1: 0.7825\n",
      "Epoch 5/20\n",
      "3751/3751 [==============================] - 0s 41us/step - loss: 0.0424 - acc: 0.8192 - auc: 0.7899 - f1: 0.8192 - val_loss: 0.0462 - val_acc: 0.7783 - val_auc: 0.7247 - val_f1: 0.7783\n",
      "Epoch 6/20\n",
      "3751/3751 [==============================] - 0s 42us/step - loss: 0.0415 - acc: 0.7131 - auc: 0.7967 - f1: 0.7131 - val_loss: 0.0460 - val_acc: 0.7249 - val_auc: 0.7333 - val_f1: 0.7249\n",
      "Epoch 7/20\n",
      "3751/3751 [==============================] - 0s 41us/step - loss: 0.0414 - acc: 0.7161 - auc: 0.7953 - f1: 0.7161 - val_loss: 0.0461 - val_acc: 0.8177 - val_auc: 0.7235 - val_f1: 0.8177\n",
      "Epoch 8/20\n",
      "3751/3751 [==============================] - 0s 40us/step - loss: 0.0409 - acc: 0.7875 - auc: 0.7998 - f1: 0.7875 - val_loss: 0.0460 - val_acc: 0.7527 - val_auc: 0.7330 - val_f1: 0.7527\n",
      "Epoch 9/20\n",
      "3751/3751 [==============================] - 0s 40us/step - loss: 0.0408 - acc: 0.7401 - auc: 0.8032 - f1: 0.7401 - val_loss: 0.0465 - val_acc: 0.8134 - val_auc: 0.7289 - val_f1: 0.8134\n",
      "Epoch 10/20\n",
      "3751/3751 [==============================] - 0s 43us/step - loss: 0.0396 - acc: 0.7817 - auc: 0.8171 - f1: 0.7817 - val_loss: 0.0459 - val_acc: 0.8038 - val_auc: 0.7380 - val_f1: 0.8038\n",
      "Epoch 11/20\n",
      "3751/3751 [==============================] - 0s 43us/step - loss: 0.0388 - acc: 0.8043 - auc: 0.8291 - f1: 0.8043 - val_loss: 0.0456 - val_acc: 0.7942 - val_auc: 0.7421 - val_f1: 0.7942\n",
      "Epoch 12/20\n",
      "3751/3751 [==============================] - 0s 42us/step - loss: 0.0388 - acc: 0.7401 - auc: 0.8289 - f1: 0.7401 - val_loss: 0.0454 - val_acc: 0.8230 - val_auc: 0.7442 - val_f1: 0.8230\n",
      "Epoch 13/20\n",
      "3751/3751 [==============================] - 0s 42us/step - loss: 0.0382 - acc: 0.7753 - auc: 0.8300 - f1: 0.7753 - val_loss: 0.0463 - val_acc: 0.8369 - val_auc: 0.7462 - val_f1: 0.8369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20\n",
      "3751/3751 [==============================] - 0s 41us/step - loss: 0.0385 - acc: 0.8083 - auc: 0.8332 - f1: 0.8083 - val_loss: 0.0456 - val_acc: 0.7132 - val_auc: 0.7522 - val_f1: 0.7132\n",
      "Epoch 15/20\n",
      "3751/3751 [==============================] - 0s 42us/step - loss: 0.0386 - acc: 0.7566 - auc: 0.8319 - f1: 0.7566 - val_loss: 0.0456 - val_acc: 0.8326 - val_auc: 0.7486 - val_f1: 0.8326\n",
      "Epoch 16/20\n",
      "3751/3751 [==============================] - 0s 42us/step - loss: 0.0365 - acc: 0.8089 - auc: 0.8426 - f1: 0.8089 - val_loss: 0.0447 - val_acc: 0.7559 - val_auc: 0.7550 - val_f1: 0.7559\n",
      "Epoch 17/20\n",
      "3751/3751 [==============================] - 0s 42us/step - loss: 0.0358 - acc: 0.7878 - auc: 0.8497 - f1: 0.7878 - val_loss: 0.0462 - val_acc: 0.8070 - val_auc: 0.7579 - val_f1: 0.8070\n",
      "Epoch 18/20\n",
      "3751/3751 [==============================] - 0s 41us/step - loss: 0.0375 - acc: 0.7614 - auc: 0.8398 - f1: 0.7614 - val_loss: 0.0490 - val_acc: 0.8529 - val_auc: 0.7526 - val_f1: 0.8529\n",
      "Epoch 19/20\n",
      "3751/3751 [==============================] - 0s 43us/step - loss: 0.0376 - acc: 0.7974 - auc: 0.8382 - f1: 0.7974 - val_loss: 0.0440 - val_acc: 0.7036 - val_auc: 0.7629 - val_f1: 0.7036\n",
      "Epoch 20/20\n",
      "3751/3751 [==============================] - 0s 42us/step - loss: 0.0379 - acc: 0.8001 - auc: 0.8390 - f1: 0.8001 - val_loss: 0.0438 - val_acc: 0.6930 - val_auc: 0.7663 - val_f1: 0.6930\n",
      "[[ 26  11]\n",
      " [277 624]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.09      0.70      0.15        37\n",
      "           1       0.98      0.69      0.81       901\n",
      "\n",
      "    accuracy                           0.69       938\n",
      "   macro avg       0.53      0.70      0.48       938\n",
      "weighted avg       0.95      0.69      0.79       938\n",
      "\n",
      "Training on fold 4/5...\n",
      "model dim:  (2, 8) 2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_10 (GRU)                 (None, 2, 256)            203520    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "gru_11 (GRU)                 (None, 2, 128)            147840    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "gru_12 (GRU)                 (None, 64)                37056     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 388,546\n",
      "Trainable params: 388,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3751 samples, validate on 938 samples\n",
      "Epoch 1/20\n",
      "3751/3751 [==============================] - 1s 397us/step - loss: 0.0467 - acc: 0.6979 - auc: 0.7123 - f1: 0.6993 - val_loss: 0.0414 - val_acc: 0.7719 - val_auc: 0.8153 - val_f1: 0.7719\n",
      "Epoch 2/20\n",
      "3751/3751 [==============================] - 0s 39us/step - loss: 0.0443 - acc: 0.7809 - auc: 0.7581 - f1: 0.7809 - val_loss: 0.0407 - val_acc: 0.7665 - val_auc: 0.8157 - val_f1: 0.7665\n",
      "Epoch 3/20\n",
      "3751/3751 [==============================] - 0s 40us/step - loss: 0.0442 - acc: 0.7387 - auc: 0.7534 - f1: 0.7387 - val_loss: 0.0409 - val_acc: 0.7537 - val_auc: 0.8172 - val_f1: 0.7537\n",
      "Epoch 4/20\n",
      "3751/3751 [==============================] - 0s 42us/step - loss: 0.0446 - acc: 0.7310 - auc: 0.7412 - f1: 0.7310 - val_loss: 0.0409 - val_acc: 0.7132 - val_auc: 0.8222 - val_f1: 0.7132\n",
      "Epoch 5/20\n",
      "3751/3751 [==============================] - 0s 40us/step - loss: 0.0425 - acc: 0.6899 - auc: 0.7861 - f1: 0.6899 - val_loss: 0.0403 - val_acc: 0.8273 - val_auc: 0.8174 - val_f1: 0.8273\n",
      "Epoch 6/20\n",
      "3751/3751 [==============================] - 0s 42us/step - loss: 0.0456 - acc: 0.8512 - auc: 0.7398 - f1: 0.8512 - val_loss: 0.0402 - val_acc: 0.6866 - val_auc: 0.8284 - val_f1: 0.6866\n",
      "Epoch 7/20\n",
      "3751/3751 [==============================] - 0s 43us/step - loss: 0.0432 - acc: 0.6828 - auc: 0.7762 - f1: 0.6828 - val_loss: 0.0388 - val_acc: 0.7836 - val_auc: 0.8294 - val_f1: 0.7836\n",
      "Epoch 8/20\n",
      "3751/3751 [==============================] - 0s 44us/step - loss: 0.0434 - acc: 0.8387 - auc: 0.7655 - f1: 0.8387 - val_loss: 0.0391 - val_acc: 0.7420 - val_auc: 0.8360 - val_f1: 0.7420\n",
      "Epoch 9/20\n",
      "3751/3751 [==============================] - 0s 43us/step - loss: 0.0413 - acc: 0.7179 - auc: 0.7904 - f1: 0.7179 - val_loss: 0.0391 - val_acc: 0.7207 - val_auc: 0.8383 - val_f1: 0.7207\n",
      "Epoch 10/20\n",
      "3751/3751 [==============================] - 0s 44us/step - loss: 0.0418 - acc: 0.7171 - auc: 0.7920 - f1: 0.7171 - val_loss: 0.0390 - val_acc: 0.7249 - val_auc: 0.8324 - val_f1: 0.7249\n",
      "Epoch 11/20\n",
      "3751/3751 [==============================] - 0s 42us/step - loss: 0.0430 - acc: 0.7689 - auc: 0.7669 - f1: 0.7689 - val_loss: 0.0397 - val_acc: 0.6514 - val_auc: 0.8384 - val_f1: 0.6514\n",
      "Epoch 12/20\n",
      "3751/3751 [==============================] - 0s 42us/step - loss: 0.0405 - acc: 0.7275 - auc: 0.7991 - f1: 0.7275 - val_loss: 0.0385 - val_acc: 0.8241 - val_auc: 0.8394 - val_f1: 0.8241\n",
      "Epoch 13/20\n",
      "3751/3751 [==============================] - 0s 44us/step - loss: 0.0408 - acc: 0.7409 - auc: 0.8096 - f1: 0.7409 - val_loss: 0.0390 - val_acc: 0.7441 - val_auc: 0.8394 - val_f1: 0.7441\n",
      "Epoch 14/20\n",
      "3751/3751 [==============================] - 0s 43us/step - loss: 0.0408 - acc: 0.7849 - auc: 0.7937 - f1: 0.7849 - val_loss: 0.0381 - val_acc: 0.8006 - val_auc: 0.8416 - val_f1: 0.8006\n",
      "Epoch 15/20\n",
      "3751/3751 [==============================] - 0s 44us/step - loss: 0.0401 - acc: 0.7110 - auc: 0.8090 - f1: 0.7110 - val_loss: 0.0378 - val_acc: 0.7953 - val_auc: 0.8373 - val_f1: 0.7953\n",
      "Epoch 16/20\n",
      "3751/3751 [==============================] - 0s 43us/step - loss: 0.0407 - acc: 0.8416 - auc: 0.8045 - f1: 0.8416 - val_loss: 0.0382 - val_acc: 0.7249 - val_auc: 0.8413 - val_f1: 0.7249\n",
      "Epoch 17/20\n",
      "3751/3751 [==============================] - 0s 44us/step - loss: 0.0403 - acc: 0.6353 - auc: 0.8127 - f1: 0.6353 - val_loss: 0.0374 - val_acc: 0.8220 - val_auc: 0.8462 - val_f1: 0.8220\n",
      "Epoch 18/20\n",
      "3751/3751 [==============================] - 0s 44us/step - loss: 0.0396 - acc: 0.8176 - auc: 0.8140 - f1: 0.8176 - val_loss: 0.0380 - val_acc: 0.6791 - val_auc: 0.8471 - val_f1: 0.6791\n",
      "Epoch 19/20\n",
      "3751/3751 [==============================] - 0s 43us/step - loss: 0.0397 - acc: 0.7313 - auc: 0.8065 - f1: 0.7313 - val_loss: 0.0370 - val_acc: 0.7953 - val_auc: 0.8463 - val_f1: 0.7953\n",
      "Epoch 20/20\n",
      "3751/3751 [==============================] - 0s 43us/step - loss: 0.0403 - acc: 0.7635 - auc: 0.8166 - f1: 0.7635 - val_loss: 0.0380 - val_acc: 0.8145 - val_auc: 0.8435 - val_f1: 0.8145\n",
      "[[ 28   9]\n",
      " [165 736]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.15      0.76      0.24        37\n",
      "           1       0.99      0.82      0.89       901\n",
      "\n",
      "    accuracy                           0.81       938\n",
      "   macro avg       0.57      0.79      0.57       938\n",
      "weighted avg       0.95      0.81      0.87       938\n",
      "\n",
      "Training on fold 5/5...\n",
      "model dim:  (2, 8) 2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_13 (GRU)                 (None, 2, 256)            203520    \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "gru_14 (GRU)                 (None, 2, 128)            147840    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "gru_15 (GRU)                 (None, 64)                37056     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 388,546\n",
      "Trainable params: 388,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3752 samples, validate on 937 samples\n",
      "Epoch 1/20\n",
      "3752/3752 [==============================] - 2s 444us/step - loss: 0.0496 - acc: 0.5680 - auc: 0.6875 - f1: 0.5699 - val_loss: 0.0420 - val_acc: 0.8463 - val_auc: 0.7628 - val_f1: 0.8463\n",
      "Epoch 2/20\n",
      "3752/3752 [==============================] - 0s 43us/step - loss: 0.0458 - acc: 0.7868 - auc: 0.7407 - f1: 0.7868 - val_loss: 0.0411 - val_acc: 0.7663 - val_auc: 0.7935 - val_f1: 0.7663\n",
      "Epoch 3/20\n",
      "3752/3752 [==============================] - 0s 40us/step - loss: 0.0442 - acc: 0.7708 - auc: 0.7471 - f1: 0.7708 - val_loss: 0.0406 - val_acc: 0.8100 - val_auc: 0.7960 - val_f1: 0.8100\n",
      "Epoch 4/20\n",
      "3752/3752 [==============================] - 0s 41us/step - loss: 0.0441 - acc: 0.7775 - auc: 0.7543 - f1: 0.7775 - val_loss: 0.0400 - val_acc: 0.7588 - val_auc: 0.8099 - val_f1: 0.7588\n",
      "Epoch 5/20\n",
      "3752/3752 [==============================] - 0s 42us/step - loss: 0.0428 - acc: 0.7433 - auc: 0.7741 - f1: 0.7433 - val_loss: 0.0392 - val_acc: 0.8015 - val_auc: 0.8103 - val_f1: 0.8015\n",
      "Epoch 6/20\n",
      "3752/3752 [==============================] - 0s 42us/step - loss: 0.0431 - acc: 0.8004 - auc: 0.7617 - f1: 0.8004 - val_loss: 0.0396 - val_acc: 0.7652 - val_auc: 0.8063 - val_f1: 0.7652\n",
      "Epoch 7/20\n",
      "3752/3752 [==============================] - 0s 41us/step - loss: 0.0423 - acc: 0.7220 - auc: 0.7909 - f1: 0.7220 - val_loss: 0.0394 - val_acc: 0.8036 - val_auc: 0.8090 - val_f1: 0.8036\n",
      "Epoch 8/20\n",
      "3752/3752 [==============================] - 0s 43us/step - loss: 0.0414 - acc: 0.7804 - auc: 0.7838 - f1: 0.7804 - val_loss: 0.0395 - val_acc: 0.7791 - val_auc: 0.8168 - val_f1: 0.7791\n",
      "Epoch 9/20\n",
      "3752/3752 [==============================] - 0s 42us/step - loss: 0.0410 - acc: 0.7641 - auc: 0.7909 - f1: 0.7641 - val_loss: 0.0402 - val_acc: 0.7930 - val_auc: 0.8030 - val_f1: 0.7930\n",
      "Epoch 10/20\n",
      "3752/3752 [==============================] - 0s 41us/step - loss: 0.0418 - acc: 0.7327 - auc: 0.7930 - f1: 0.7327 - val_loss: 0.0399 - val_acc: 0.8015 - val_auc: 0.8096 - val_f1: 0.8015\n",
      "Epoch 11/20\n",
      "3752/3752 [==============================] - 0s 42us/step - loss: 0.0403 - acc: 0.7535 - auc: 0.8019 - f1: 0.7535 - val_loss: 0.0400 - val_acc: 0.8058 - val_auc: 0.8088 - val_f1: 0.8058\n",
      "Epoch 12/20\n",
      "3752/3752 [==============================] - 0s 44us/step - loss: 0.0402 - acc: 0.7807 - auc: 0.8136 - f1: 0.7807 - val_loss: 0.0390 - val_acc: 0.7844 - val_auc: 0.8166 - val_f1: 0.7844\n",
      "Epoch 13/20\n",
      "3752/3752 [==============================] - 0s 43us/step - loss: 0.0410 - acc: 0.7217 - auc: 0.7961 - f1: 0.7217 - val_loss: 0.0393 - val_acc: 0.8581 - val_auc: 0.8206 - val_f1: 0.8581\n",
      "Epoch 14/20\n",
      "3752/3752 [==============================] - 0s 45us/step - loss: 0.0392 - acc: 0.7950 - auc: 0.8159 - f1: 0.7950 - val_loss: 0.0400 - val_acc: 0.7940 - val_auc: 0.8114 - val_f1: 0.7940\n",
      "Epoch 15/20\n",
      "3752/3752 [==============================] - 0s 45us/step - loss: 0.0392 - acc: 0.7348 - auc: 0.8166 - f1: 0.7348 - val_loss: 0.0422 - val_acc: 0.8986 - val_auc: 0.8109 - val_f1: 0.8986\n",
      "Epoch 16/20\n",
      "3752/3752 [==============================] - 0s 44us/step - loss: 0.0404 - acc: 0.8289 - auc: 0.8023 - f1: 0.8289 - val_loss: 0.0409 - val_acc: 0.6585 - val_auc: 0.7969 - val_f1: 0.6585\n",
      "Epoch 17/20\n",
      "3752/3752 [==============================] - 0s 44us/step - loss: 0.0386 - acc: 0.7537 - auc: 0.8246 - f1: 0.7537 - val_loss: 0.0408 - val_acc: 0.8986 - val_auc: 0.8099 - val_f1: 0.8986\n",
      "Epoch 18/20\n",
      "3752/3752 [==============================] - 0s 45us/step - loss: 0.0389 - acc: 0.7838 - auc: 0.8222 - f1: 0.7838 - val_loss: 0.0385 - val_acc: 0.7780 - val_auc: 0.8248 - val_f1: 0.7780\n",
      "Epoch 19/20\n",
      "3752/3752 [==============================] - 0s 44us/step - loss: 0.0380 - acc: 0.7575 - auc: 0.8317 - f1: 0.7575 - val_loss: 0.0400 - val_acc: 0.8527 - val_auc: 0.8226 - val_f1: 0.8527\n",
      "Epoch 20/20\n",
      "3752/3752 [==============================] - 0s 43us/step - loss: 0.0378 - acc: 0.8241 - auc: 0.8289 - f1: 0.8241 - val_loss: 0.0394 - val_acc: 0.7311 - val_auc: 0.8170 - val_f1: 0.7311\n",
      "[[ 26  10]\n",
      " [242 659]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.72      0.17        36\n",
      "           1       0.99      0.73      0.84       901\n",
      "\n",
      "    accuracy                           0.73       937\n",
      "   macro avg       0.54      0.73      0.51       937\n",
      "weighted avg       0.95      0.73      0.81       937\n",
      "\n",
      "time (in seconds) 29.54588747024536\n",
      "... DONE!\n",
      "... DONE!\n",
      "window_size: 3\n",
      "generating tensor...\n",
      "...DONE!\n",
      "tensor dimensions:\n",
      "tensor input X: (5015, 8, 3)\n",
      "tensor input y: (5015, 2)\n",
      "proportion of y labels: (array([0, 1]), array([ 221, 4794]))\n",
      "Splitting dataset into Train and Test sets...\n",
      "Tensor X train: (3510, 3, 8)\n",
      "Tensor y train: (3510, 2)\n",
      "Tensor X test: (1505, 3, 8)\n",
      "Tensor y test: (1505, 2)\n",
      "computing weights...\n",
      "... DONE!\n",
      "setting stratified k-fold...\n",
      "number of k: 5\n",
      "... DONE!\n",
      "Executing algorithm...\n",
      "number of epochs: 20\n",
      "number of batch: 512\n",
      "Training on fold 1/5...\n",
      "model dim:  (3, 8) 2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_16 (GRU)                 (None, 3, 256)            203520    \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 3, 256)            0         \n",
      "_________________________________________________________________\n",
      "gru_17 (GRU)                 (None, 3, 128)            147840    \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 3, 128)            0         \n",
      "_________________________________________________________________\n",
      "gru_18 (GRU)                 (None, 64)                37056     \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 388,546\n",
      "Trainable params: 388,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2807 samples, validate on 703 samples\n",
      "Epoch 1/20\n",
      "2807/2807 [==============================] - 2s 667us/step - loss: 0.0505 - acc: 0.6801 - auc: 0.7088 - f1: 0.6818 - val_loss: 0.0421 - val_acc: 0.7809 - val_auc: 0.8343 - val_f1: 0.7809\n",
      "Epoch 2/20\n",
      "2807/2807 [==============================] - 0s 55us/step - loss: 0.0480 - acc: 0.7168 - auc: 0.7531 - f1: 0.7168 - val_loss: 0.0402 - val_acc: 0.8122 - val_auc: 0.8562 - val_f1: 0.8122\n",
      "Epoch 3/20\n",
      "2807/2807 [==============================] - 0s 53us/step - loss: 0.0468 - acc: 0.7727 - auc: 0.7506 - f1: 0.7727 - val_loss: 0.0407 - val_acc: 0.6913 - val_auc: 0.8734 - val_f1: 0.6913\n",
      "Epoch 4/20\n",
      "2807/2807 [==============================] - 0s 54us/step - loss: 0.0455 - acc: 0.6847 - auc: 0.7700 - f1: 0.6847 - val_loss: 0.0391 - val_acc: 0.8094 - val_auc: 0.8606 - val_f1: 0.8094\n",
      "Epoch 5/20\n",
      "2807/2807 [==============================] - 0s 57us/step - loss: 0.0466 - acc: 0.8133 - auc: 0.7653 - f1: 0.8133 - val_loss: 0.0396 - val_acc: 0.7724 - val_auc: 0.8598 - val_f1: 0.7724\n",
      "Epoch 6/20\n",
      "2807/2807 [==============================] - 0s 58us/step - loss: 0.0450 - acc: 0.7007 - auc: 0.7860 - f1: 0.7007 - val_loss: 0.0397 - val_acc: 0.7226 - val_auc: 0.8730 - val_f1: 0.7226\n",
      "Epoch 7/20\n",
      "2807/2807 [==============================] - 0s 56us/step - loss: 0.0449 - acc: 0.7577 - auc: 0.7841 - f1: 0.7577 - val_loss: 0.0382 - val_acc: 0.7881 - val_auc: 0.8784 - val_f1: 0.7881\n",
      "Epoch 8/20\n",
      "2807/2807 [==============================] - 0s 58us/step - loss: 0.0427 - acc: 0.7460 - auc: 0.8019 - f1: 0.7460 - val_loss: 0.0382 - val_acc: 0.7283 - val_auc: 0.8818 - val_f1: 0.7283\n",
      "Epoch 9/20\n",
      "2807/2807 [==============================] - 0s 55us/step - loss: 0.0417 - acc: 0.7599 - auc: 0.8224 - f1: 0.7599 - val_loss: 0.0372 - val_acc: 0.7952 - val_auc: 0.8790 - val_f1: 0.7952\n",
      "Epoch 10/20\n",
      "2807/2807 [==============================] - 0s 58us/step - loss: 0.0427 - acc: 0.7592 - auc: 0.8061 - f1: 0.7592 - val_loss: 0.0372 - val_acc: 0.7454 - val_auc: 0.8845 - val_f1: 0.7454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20\n",
      "2807/2807 [==============================] - 0s 58us/step - loss: 0.0422 - acc: 0.7791 - auc: 0.8086 - f1: 0.7791 - val_loss: 0.0383 - val_acc: 0.7141 - val_auc: 0.8786 - val_f1: 0.7141\n",
      "Epoch 12/20\n",
      "2807/2807 [==============================] - 0s 60us/step - loss: 0.0426 - acc: 0.6494 - auc: 0.8231 - f1: 0.6494 - val_loss: 0.0370 - val_acc: 0.8165 - val_auc: 0.8733 - val_f1: 0.8165\n",
      "Epoch 13/20\n",
      "2807/2807 [==============================] - 0s 57us/step - loss: 0.0408 - acc: 0.7781 - auc: 0.8087 - f1: 0.7781 - val_loss: 0.0370 - val_acc: 0.7511 - val_auc: 0.8783 - val_f1: 0.7511\n",
      "Epoch 14/20\n",
      "2807/2807 [==============================] - 0s 58us/step - loss: 0.0402 - acc: 0.7781 - auc: 0.8370 - f1: 0.7781 - val_loss: 0.0367 - val_acc: 0.7340 - val_auc: 0.8808 - val_f1: 0.7340\n",
      "Epoch 15/20\n",
      "2807/2807 [==============================] - 0s 60us/step - loss: 0.0381 - acc: 0.7442 - auc: 0.8498 - f1: 0.7442 - val_loss: 0.0366 - val_acc: 0.8051 - val_auc: 0.8757 - val_f1: 0.8051\n",
      "Epoch 16/20\n",
      "2807/2807 [==============================] - 0s 58us/step - loss: 0.0387 - acc: 0.7659 - auc: 0.8465 - f1: 0.7659 - val_loss: 0.0370 - val_acc: 0.7710 - val_auc: 0.8739 - val_f1: 0.7710\n",
      "Epoch 17/20\n",
      "2807/2807 [==============================] - 0s 57us/step - loss: 0.0386 - acc: 0.7798 - auc: 0.8444 - f1: 0.7798 - val_loss: 0.0378 - val_acc: 0.7198 - val_auc: 0.8695 - val_f1: 0.7198\n",
      "Epoch 18/20\n",
      "2807/2807 [==============================] - 0s 59us/step - loss: 0.0381 - acc: 0.7036 - auc: 0.8527 - f1: 0.7036 - val_loss: 0.0395 - val_acc: 0.8606 - val_auc: 0.8669 - val_f1: 0.8606\n",
      "Epoch 19/20\n",
      "2807/2807 [==============================] - 0s 60us/step - loss: 0.0397 - acc: 0.7966 - auc: 0.8384 - f1: 0.7966 - val_loss: 0.0385 - val_acc: 0.6671 - val_auc: 0.8769 - val_f1: 0.6671\n",
      "Epoch 20/20\n",
      "2807/2807 [==============================] - 0s 60us/step - loss: 0.0389 - acc: 0.7399 - auc: 0.8443 - f1: 0.7399 - val_loss: 0.0377 - val_acc: 0.8023 - val_auc: 0.8702 - val_f1: 0.8023\n",
      "[[ 23   7]\n",
      " [132 541]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.15      0.77      0.25        30\n",
      "           1       0.99      0.80      0.89       673\n",
      "\n",
      "    accuracy                           0.80       703\n",
      "   macro avg       0.57      0.79      0.57       703\n",
      "weighted avg       0.95      0.80      0.86       703\n",
      "\n",
      "Training on fold 2/5...\n",
      "model dim:  (3, 8) 2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_19 (GRU)                 (None, 3, 256)            203520    \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 3, 256)            0         \n",
      "_________________________________________________________________\n",
      "gru_20 (GRU)                 (None, 3, 128)            147840    \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 3, 128)            0         \n",
      "_________________________________________________________________\n",
      "gru_21 (GRU)                 (None, 64)                37056     \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 388,546\n",
      "Trainable params: 388,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2808 samples, validate on 702 samples\n",
      "Epoch 1/20\n",
      "2808/2808 [==============================] - 2s 733us/step - loss: 0.0496 - acc: 0.6724 - auc: 0.7203 - f1: 0.6732 - val_loss: 0.0498 - val_acc: 0.7194 - val_auc: 0.7535 - val_f1: 0.7194\n",
      "Epoch 2/20\n",
      "2808/2808 [==============================] - 0s 53us/step - loss: 0.0458 - acc: 0.7415 - auc: 0.7904 - f1: 0.7415 - val_loss: 0.0485 - val_acc: 0.7963 - val_auc: 0.7344 - val_f1: 0.7963\n",
      "Epoch 3/20\n",
      "2808/2808 [==============================] - 0s 57us/step - loss: 0.0441 - acc: 0.7525 - auc: 0.7978 - f1: 0.7525 - val_loss: 0.0491 - val_acc: 0.7379 - val_auc: 0.7223 - val_f1: 0.7379\n",
      "Epoch 4/20\n",
      "2808/2808 [==============================] - 0s 62us/step - loss: 0.0437 - acc: 0.7347 - auc: 0.7993 - f1: 0.7347 - val_loss: 0.0514 - val_acc: 0.7749 - val_auc: 0.7179 - val_f1: 0.7749\n",
      "Epoch 5/20\n",
      "2808/2808 [==============================] - 0s 58us/step - loss: 0.0436 - acc: 0.7625 - auc: 0.8080 - f1: 0.7625 - val_loss: 0.0522 - val_acc: 0.6766 - val_auc: 0.7273 - val_f1: 0.6766\n",
      "Epoch 6/20\n",
      "2808/2808 [==============================] - 0s 58us/step - loss: 0.0404 - acc: 0.7098 - auc: 0.8399 - f1: 0.7098 - val_loss: 0.0490 - val_acc: 0.8262 - val_auc: 0.7328 - val_f1: 0.8262\n",
      "Epoch 7/20\n",
      "2808/2808 [==============================] - 0s 59us/step - loss: 0.0419 - acc: 0.7742 - auc: 0.8185 - f1: 0.7742 - val_loss: 0.0489 - val_acc: 0.7293 - val_auc: 0.7541 - val_f1: 0.7293\n",
      "Epoch 8/20\n",
      "2808/2808 [==============================] - 0s 60us/step - loss: 0.0424 - acc: 0.7222 - auc: 0.8211 - f1: 0.7222 - val_loss: 0.0465 - val_acc: 0.7963 - val_auc: 0.7671 - val_f1: 0.7963\n",
      "Epoch 9/20\n",
      "2808/2808 [==============================] - 0s 61us/step - loss: 0.0416 - acc: 0.6688 - auc: 0.8396 - f1: 0.6688 - val_loss: 0.0479 - val_acc: 0.7906 - val_auc: 0.7571 - val_f1: 0.7906\n",
      "Epoch 10/20\n",
      "2808/2808 [==============================] - 0s 60us/step - loss: 0.0419 - acc: 0.8166 - auc: 0.8274 - f1: 0.8166 - val_loss: 0.0526 - val_acc: 0.6652 - val_auc: 0.7405 - val_f1: 0.6652\n",
      "Epoch 11/20\n",
      "2808/2808 [==============================] - 0s 60us/step - loss: 0.0412 - acc: 0.6496 - auc: 0.8416 - f1: 0.6496 - val_loss: 0.0519 - val_acc: 0.8006 - val_auc: 0.7441 - val_f1: 0.8006\n",
      "Epoch 12/20\n",
      "2808/2808 [==============================] - 0s 62us/step - loss: 0.0409 - acc: 0.8159 - auc: 0.8366 - f1: 0.8159 - val_loss: 0.0499 - val_acc: 0.7208 - val_auc: 0.7453 - val_f1: 0.7208\n",
      "Epoch 13/20\n",
      "2808/2808 [==============================] - 0s 62us/step - loss: 0.0395 - acc: 0.6684 - auc: 0.8432 - f1: 0.6684 - val_loss: 0.0503 - val_acc: 0.7792 - val_auc: 0.7493 - val_f1: 0.7792\n",
      "Epoch 14/20\n",
      "2808/2808 [==============================] - 0s 59us/step - loss: 0.0396 - acc: 0.7984 - auc: 0.8454 - f1: 0.7984 - val_loss: 0.0524 - val_acc: 0.6738 - val_auc: 0.7415 - val_f1: 0.6738\n",
      "Epoch 15/20\n",
      "2808/2808 [==============================] - 0s 61us/step - loss: 0.0368 - acc: 0.7058 - auc: 0.8701 - f1: 0.7058 - val_loss: 0.0542 - val_acc: 0.8020 - val_auc: 0.7366 - val_f1: 0.8020\n",
      "Epoch 16/20\n",
      "2808/2808 [==============================] - 0s 58us/step - loss: 0.0372 - acc: 0.7838 - auc: 0.8605 - f1: 0.7838 - val_loss: 0.0560 - val_acc: 0.6638 - val_auc: 0.7377 - val_f1: 0.6638\n",
      "Epoch 17/20\n",
      "2808/2808 [==============================] - 0s 61us/step - loss: 0.0365 - acc: 0.7561 - auc: 0.8691 - f1: 0.7561 - val_loss: 0.0570 - val_acc: 0.8048 - val_auc: 0.7502 - val_f1: 0.8048\n",
      "Epoch 18/20\n",
      "2808/2808 [==============================] - 0s 62us/step - loss: 0.0364 - acc: 0.7240 - auc: 0.8677 - f1: 0.7240 - val_loss: 0.0527 - val_acc: 0.7137 - val_auc: 0.7511 - val_f1: 0.7137\n",
      "Epoch 19/20\n",
      "2808/2808 [==============================] - 0s 60us/step - loss: 0.0370 - acc: 0.7831 - auc: 0.8608 - f1: 0.7831 - val_loss: 0.0562 - val_acc: 0.7721 - val_auc: 0.7513 - val_f1: 0.7721\n",
      "Epoch 20/20\n",
      "2808/2808 [==============================] - 0s 60us/step - loss: 0.0354 - acc: 0.7393 - auc: 0.8722 - f1: 0.7393 - val_loss: 0.0598 - val_acc: 0.8177 - val_auc: 0.7591 - val_f1: 0.8177\n",
      "[[ 16  13]\n",
      " [115 558]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.55      0.20        29\n",
      "           1       0.98      0.83      0.90       673\n",
      "\n",
      "    accuracy                           0.82       702\n",
      "   macro avg       0.55      0.69      0.55       702\n",
      "weighted avg       0.94      0.82      0.87       702\n",
      "\n",
      "Training on fold 3/5...\n",
      "model dim:  (3, 8) 2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_22 (GRU)                 (None, 3, 256)            203520    \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 3, 256)            0         \n",
      "_________________________________________________________________\n",
      "gru_23 (GRU)                 (None, 3, 128)            147840    \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 3, 128)            0         \n",
      "_________________________________________________________________\n",
      "gru_24 (GRU)                 (None, 64)                37056     \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 388,546\n",
      "Trainable params: 388,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2808 samples, validate on 702 samples\n",
      "Epoch 1/20\n",
      "2808/2808 [==============================] - 2s 809us/step - loss: 0.0502 - acc: 0.6474 - auc: 0.6998 - f1: 0.6477 - val_loss: 0.0482 - val_acc: 0.7778 - val_auc: 0.7391 - val_f1: 0.7778\n",
      "Epoch 2/20\n",
      "2808/2808 [==============================] - 0s 53us/step - loss: 0.0479 - acc: 0.7977 - auc: 0.7627 - f1: 0.7977 - val_loss: 0.0460 - val_acc: 0.7208 - val_auc: 0.7967 - val_f1: 0.7208\n",
      "Epoch 3/20\n",
      "2808/2808 [==============================] - 0s 57us/step - loss: 0.0445 - acc: 0.7532 - auc: 0.7811 - f1: 0.7532 - val_loss: 0.0451 - val_acc: 0.8077 - val_auc: 0.7848 - val_f1: 0.8077\n",
      "Epoch 4/20\n",
      "2808/2808 [==============================] - 0s 59us/step - loss: 0.0451 - acc: 0.7938 - auc: 0.7697 - f1: 0.7938 - val_loss: 0.0459 - val_acc: 0.6980 - val_auc: 0.7835 - val_f1: 0.6980\n",
      "Epoch 5/20\n",
      "2808/2808 [==============================] - 0s 58us/step - loss: 0.0444 - acc: 0.7115 - auc: 0.7947 - f1: 0.7115 - val_loss: 0.0453 - val_acc: 0.7678 - val_auc: 0.7781 - val_f1: 0.7678\n",
      "Epoch 6/20\n",
      "2808/2808 [==============================] - 0s 58us/step - loss: 0.0436 - acc: 0.7967 - auc: 0.7885 - f1: 0.7967 - val_loss: 0.0444 - val_acc: 0.7607 - val_auc: 0.7926 - val_f1: 0.7607\n",
      "Epoch 7/20\n",
      "2808/2808 [==============================] - 0s 58us/step - loss: 0.0417 - acc: 0.7667 - auc: 0.8144 - f1: 0.7667 - val_loss: 0.0436 - val_acc: 0.7635 - val_auc: 0.7993 - val_f1: 0.7635\n",
      "Epoch 8/20\n",
      "2808/2808 [==============================] - 0s 58us/step - loss: 0.0424 - acc: 0.8266 - auc: 0.8228 - f1: 0.8266 - val_loss: 0.0439 - val_acc: 0.7265 - val_auc: 0.7984 - val_f1: 0.7265\n",
      "Epoch 9/20\n",
      "2808/2808 [==============================] - 0s 59us/step - loss: 0.0441 - acc: 0.6257 - auc: 0.8174 - f1: 0.6257 - val_loss: 0.0436 - val_acc: 0.7493 - val_auc: 0.8027 - val_f1: 0.7493\n",
      "Epoch 10/20\n",
      "2808/2808 [==============================] - 0s 61us/step - loss: 0.0434 - acc: 0.8458 - auc: 0.8150 - f1: 0.8458 - val_loss: 0.0437 - val_acc: 0.7934 - val_auc: 0.8026 - val_f1: 0.7934\n",
      "Epoch 11/20\n",
      "2808/2808 [==============================] - 0s 60us/step - loss: 0.0420 - acc: 0.7172 - auc: 0.8317 - f1: 0.7172 - val_loss: 0.0430 - val_acc: 0.7322 - val_auc: 0.8174 - val_f1: 0.7322\n",
      "Epoch 12/20\n",
      "2808/2808 [==============================] - 0s 60us/step - loss: 0.0405 - acc: 0.8166 - auc: 0.8322 - f1: 0.8166 - val_loss: 0.0437 - val_acc: 0.8219 - val_auc: 0.8106 - val_f1: 0.8219\n",
      "Epoch 13/20\n",
      "2808/2808 [==============================] - 0s 61us/step - loss: 0.0406 - acc: 0.7657 - auc: 0.8131 - f1: 0.7657 - val_loss: 0.0422 - val_acc: 0.7037 - val_auc: 0.8195 - val_f1: 0.7037\n",
      "Epoch 14/20\n",
      "2808/2808 [==============================] - 0s 60us/step - loss: 0.0402 - acc: 0.7721 - auc: 0.8352 - f1: 0.7721 - val_loss: 0.0444 - val_acc: 0.8219 - val_auc: 0.8086 - val_f1: 0.8219\n",
      "Epoch 15/20\n",
      "2808/2808 [==============================] - 0s 61us/step - loss: 0.0393 - acc: 0.7995 - auc: 0.8460 - f1: 0.7995 - val_loss: 0.0425 - val_acc: 0.7507 - val_auc: 0.8152 - val_f1: 0.7507\n",
      "Epoch 16/20\n",
      "2808/2808 [==============================] - 0s 60us/step - loss: 0.0393 - acc: 0.7764 - auc: 0.8398 - f1: 0.7764 - val_loss: 0.0417 - val_acc: 0.7977 - val_auc: 0.8243 - val_f1: 0.7977\n",
      "Epoch 17/20\n",
      "2808/2808 [==============================] - 0s 61us/step - loss: 0.0382 - acc: 0.8095 - auc: 0.8483 - f1: 0.8095 - val_loss: 0.0420 - val_acc: 0.7764 - val_auc: 0.8227 - val_f1: 0.7764\n",
      "Epoch 18/20\n",
      "2808/2808 [==============================] - 0s 61us/step - loss: 0.0375 - acc: 0.7724 - auc: 0.8543 - f1: 0.7724 - val_loss: 0.0430 - val_acc: 0.8134 - val_auc: 0.8254 - val_f1: 0.8134\n",
      "Epoch 19/20\n",
      "2808/2808 [==============================] - 0s 61us/step - loss: 0.0380 - acc: 0.8152 - auc: 0.8492 - f1: 0.8152 - val_loss: 0.0408 - val_acc: 0.7821 - val_auc: 0.8380 - val_f1: 0.7821\n",
      "Epoch 20/20\n",
      "2808/2808 [==============================] - 0s 61us/step - loss: 0.0371 - acc: 0.7999 - auc: 0.8525 - f1: 0.7999 - val_loss: 0.0416 - val_acc: 0.7778 - val_auc: 0.8274 - val_f1: 0.7778\n",
      "[[ 20   9]\n",
      " [147 526]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.69      0.20        29\n",
      "           1       0.98      0.78      0.87       673\n",
      "\n",
      "    accuracy                           0.78       702\n",
      "   macro avg       0.55      0.74      0.54       702\n",
      "weighted avg       0.95      0.78      0.84       702\n",
      "\n",
      "Training on fold 4/5...\n",
      "model dim:  (3, 8) 2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_25 (GRU)                 (None, 3, 256)            203520    \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 3, 256)            0         \n",
      "_________________________________________________________________\n",
      "gru_26 (GRU)                 (None, 3, 128)            147840    \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 3, 128)            0         \n",
      "_________________________________________________________________\n",
      "gru_27 (GRU)                 (None, 64)                37056     \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 388,546\n",
      "Trainable params: 388,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2808 samples, validate on 702 samples\n",
      "Epoch 1/20\n",
      "2808/2808 [==============================] - 3s 935us/step - loss: 0.0508 - acc: 0.6026 - auc: 0.7045 - f1: 0.6042 - val_loss: 0.0515 - val_acc: 0.7977 - val_auc: 0.6972 - val_f1: 0.7977\n",
      "Epoch 2/20\n",
      "2808/2808 [==============================] - 0s 62us/step - loss: 0.0447 - acc: 0.7824 - auc: 0.7756 - f1: 0.7824 - val_loss: 0.0500 - val_acc: 0.7778 - val_auc: 0.7306 - val_f1: 0.7778\n",
      "Epoch 3/20\n",
      "2808/2808 [==============================] - 0s 63us/step - loss: 0.0447 - acc: 0.7910 - auc: 0.7764 - f1: 0.7910 - val_loss: 0.0476 - val_acc: 0.7977 - val_auc: 0.7515 - val_f1: 0.7977\n",
      "Epoch 4/20\n",
      "2808/2808 [==============================] - 0s 63us/step - loss: 0.0438 - acc: 0.7835 - auc: 0.7962 - f1: 0.7835 - val_loss: 0.0465 - val_acc: 0.7350 - val_auc: 0.7744 - val_f1: 0.7350\n",
      "Epoch 5/20\n",
      "2808/2808 [==============================] - 0s 60us/step - loss: 0.0426 - acc: 0.7436 - auc: 0.8216 - f1: 0.7436 - val_loss: 0.0489 - val_acc: 0.8063 - val_auc: 0.7551 - val_f1: 0.8063\n",
      "Epoch 6/20\n",
      "2808/2808 [==============================] - 0s 61us/step - loss: 0.0445 - acc: 0.8273 - auc: 0.8070 - f1: 0.8273 - val_loss: 0.0469 - val_acc: 0.7322 - val_auc: 0.7757 - val_f1: 0.7322\n",
      "Epoch 7/20\n",
      "2808/2808 [==============================] - 0s 61us/step - loss: 0.0429 - acc: 0.7123 - auc: 0.8156 - f1: 0.7123 - val_loss: 0.0470 - val_acc: 0.7949 - val_auc: 0.7689 - val_f1: 0.7949\n",
      "Epoch 8/20\n",
      "2808/2808 [==============================] - 0s 58us/step - loss: 0.0437 - acc: 0.8465 - auc: 0.8098 - f1: 0.8465 - val_loss: 0.0460 - val_acc: 0.7493 - val_auc: 0.7772 - val_f1: 0.7493\n",
      "Epoch 9/20\n",
      "2808/2808 [==============================] - 0s 60us/step - loss: 0.0429 - acc: 0.6731 - auc: 0.8317 - f1: 0.6731 - val_loss: 0.0459 - val_acc: 0.7407 - val_auc: 0.7791 - val_f1: 0.7407\n",
      "Epoch 10/20\n",
      "2808/2808 [==============================] - 0s 60us/step - loss: 0.0422 - acc: 0.8187 - auc: 0.8116 - f1: 0.8187 - val_loss: 0.0477 - val_acc: 0.8177 - val_auc: 0.7742 - val_f1: 0.8177\n",
      "Epoch 11/20\n",
      "2808/2808 [==============================] - 0s 61us/step - loss: 0.0406 - acc: 0.7938 - auc: 0.8310 - f1: 0.7938 - val_loss: 0.0443 - val_acc: 0.7080 - val_auc: 0.7905 - val_f1: 0.7080\n",
      "Epoch 12/20\n",
      "2808/2808 [==============================] - 0s 60us/step - loss: 0.0410 - acc: 0.7318 - auc: 0.8338 - f1: 0.7318 - val_loss: 0.0455 - val_acc: 0.8020 - val_auc: 0.7804 - val_f1: 0.8020\n",
      "Epoch 13/20\n",
      "2808/2808 [==============================] - 0s 60us/step - loss: 0.0404 - acc: 0.8173 - auc: 0.8367 - f1: 0.8173 - val_loss: 0.0460 - val_acc: 0.7721 - val_auc: 0.7838 - val_f1: 0.7721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20\n",
      "2808/2808 [==============================] - 0s 62us/step - loss: 0.0397 - acc: 0.7881 - auc: 0.8434 - f1: 0.7881 - val_loss: 0.0460 - val_acc: 0.7536 - val_auc: 0.7858 - val_f1: 0.7536\n",
      "Epoch 15/20\n",
      "2808/2808 [==============================] - 0s 65us/step - loss: 0.0389 - acc: 0.7760 - auc: 0.8350 - f1: 0.7760 - val_loss: 0.0446 - val_acc: 0.7906 - val_auc: 0.7934 - val_f1: 0.7906\n",
      "Epoch 16/20\n",
      "2808/2808 [==============================] - 0s 66us/step - loss: 0.0380 - acc: 0.7863 - auc: 0.8555 - f1: 0.7863 - val_loss: 0.0443 - val_acc: 0.7436 - val_auc: 0.7891 - val_f1: 0.7436\n",
      "Epoch 17/20\n",
      "2808/2808 [==============================] - 0s 71us/step - loss: 0.0380 - acc: 0.7593 - auc: 0.8548 - f1: 0.7593 - val_loss: 0.0456 - val_acc: 0.7678 - val_auc: 0.7894 - val_f1: 0.7678\n",
      "Epoch 18/20\n",
      "2808/2808 [==============================] - 0s 68us/step - loss: 0.0378 - acc: 0.7692 - auc: 0.8553 - f1: 0.7692 - val_loss: 0.0468 - val_acc: 0.8148 - val_auc: 0.7929 - val_f1: 0.8148\n",
      "Epoch 19/20\n",
      "2808/2808 [==============================] - 0s 69us/step - loss: 0.0389 - acc: 0.7699 - auc: 0.8447 - f1: 0.7699 - val_loss: 0.0451 - val_acc: 0.7564 - val_auc: 0.7974 - val_f1: 0.7564\n",
      "Epoch 20/20\n",
      "2808/2808 [==============================] - 0s 68us/step - loss: 0.0379 - acc: 0.8173 - auc: 0.8570 - f1: 0.8173 - val_loss: 0.0429 - val_acc: 0.7493 - val_auc: 0.8088 - val_f1: 0.7493\n",
      "[[ 22   7]\n",
      " [169 504]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.76      0.20        29\n",
      "           1       0.99      0.75      0.85       673\n",
      "\n",
      "    accuracy                           0.75       702\n",
      "   macro avg       0.55      0.75      0.53       702\n",
      "weighted avg       0.95      0.75      0.82       702\n",
      "\n",
      "Training on fold 5/5...\n",
      "model dim:  (3, 8) 2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_28 (GRU)                 (None, 3, 256)            203520    \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 3, 256)            0         \n",
      "_________________________________________________________________\n",
      "gru_29 (GRU)                 (None, 3, 128)            147840    \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 3, 128)            0         \n",
      "_________________________________________________________________\n",
      "gru_30 (GRU)                 (None, 64)                37056     \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 388,546\n",
      "Trainable params: 388,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2809 samples, validate on 701 samples\n",
      "Epoch 1/20\n",
      "2809/2809 [==============================] - 3s 1ms/step - loss: 0.0516 - acc: 0.6372 - auc: 0.6797 - f1: 0.6381 - val_loss: 0.0468 - val_acc: 0.7432 - val_auc: 0.7783 - val_f1: 0.7432\n",
      "Epoch 2/20\n",
      "2809/2809 [==============================] - 0s 62us/step - loss: 0.0483 - acc: 0.8117 - auc: 0.7491 - f1: 0.8117 - val_loss: 0.0434 - val_acc: 0.7803 - val_auc: 0.8073 - val_f1: 0.7803\n",
      "Epoch 3/20\n",
      "2809/2809 [==============================] - 0s 63us/step - loss: 0.0468 - acc: 0.6679 - auc: 0.7902 - f1: 0.6679 - val_loss: 0.0430 - val_acc: 0.7718 - val_auc: 0.8014 - val_f1: 0.7718\n",
      "Epoch 4/20\n",
      "2809/2809 [==============================] - 0s 63us/step - loss: 0.0461 - acc: 0.8078 - auc: 0.7669 - f1: 0.8078 - val_loss: 0.0433 - val_acc: 0.7846 - val_auc: 0.7966 - val_f1: 0.7846\n",
      "Epoch 5/20\n",
      "2809/2809 [==============================] - 0s 69us/step - loss: 0.0433 - acc: 0.7640 - auc: 0.7955 - f1: 0.7640 - val_loss: 0.0429 - val_acc: 0.7375 - val_auc: 0.8153 - val_f1: 0.7375\n",
      "Epoch 6/20\n",
      "2809/2809 [==============================] - 0s 64us/step - loss: 0.0438 - acc: 0.7423 - auc: 0.7970 - f1: 0.7423 - val_loss: 0.0421 - val_acc: 0.7903 - val_auc: 0.8103 - val_f1: 0.7903\n",
      "Epoch 7/20\n",
      "2809/2809 [==============================] - 0s 64us/step - loss: 0.0430 - acc: 0.7889 - auc: 0.8020 - f1: 0.7889 - val_loss: 0.0416 - val_acc: 0.8074 - val_auc: 0.8143 - val_f1: 0.8074\n",
      "Epoch 8/20\n",
      "2809/2809 [==============================] - 0s 64us/step - loss: 0.0429 - acc: 0.7383 - auc: 0.8243 - f1: 0.7383 - val_loss: 0.0414 - val_acc: 0.8074 - val_auc: 0.8145 - val_f1: 0.8074\n",
      "Epoch 9/20\n",
      "2809/2809 [==============================] - 0s 62us/step - loss: 0.0429 - acc: 0.7839 - auc: 0.8044 - f1: 0.7839 - val_loss: 0.0416 - val_acc: 0.7489 - val_auc: 0.8144 - val_f1: 0.7489\n",
      "Epoch 10/20\n",
      "2809/2809 [==============================] - 0s 63us/step - loss: 0.0430 - acc: 0.7198 - auc: 0.8048 - f1: 0.7198 - val_loss: 0.0418 - val_acc: 0.7660 - val_auc: 0.8076 - val_f1: 0.7660\n",
      "Epoch 11/20\n",
      "2809/2809 [==============================] - 0s 64us/step - loss: 0.0414 - acc: 0.7757 - auc: 0.8139 - f1: 0.7757 - val_loss: 0.0411 - val_acc: 0.8146 - val_auc: 0.8115 - val_f1: 0.8146\n",
      "Epoch 12/20\n",
      "2809/2809 [==============================] - 0s 62us/step - loss: 0.0425 - acc: 0.7376 - auc: 0.8220 - f1: 0.7376 - val_loss: 0.0411 - val_acc: 0.8459 - val_auc: 0.8060 - val_f1: 0.8459\n",
      "Epoch 13/20\n",
      "2809/2809 [==============================] - 0s 66us/step - loss: 0.0410 - acc: 0.8095 - auc: 0.8298 - f1: 0.8095 - val_loss: 0.0408 - val_acc: 0.7589 - val_auc: 0.8120 - val_f1: 0.7589\n",
      "Epoch 14/20\n",
      "2809/2809 [==============================] - 0s 64us/step - loss: 0.0413 - acc: 0.8038 - auc: 0.8235 - f1: 0.8038 - val_loss: 0.0421 - val_acc: 0.7090 - val_auc: 0.8112 - val_f1: 0.7090\n",
      "Epoch 15/20\n",
      "2809/2809 [==============================] - 0s 64us/step - loss: 0.0420 - acc: 0.6778 - auc: 0.8293 - f1: 0.6778 - val_loss: 0.0440 - val_acc: 0.9044 - val_auc: 0.8000 - val_f1: 0.9044\n",
      "Epoch 16/20\n",
      "2809/2809 [==============================] - 0s 68us/step - loss: 0.0437 - acc: 0.8423 - auc: 0.8376 - f1: 0.8423 - val_loss: 0.0425 - val_acc: 0.6419 - val_auc: 0.8192 - val_f1: 0.6419\n",
      "Epoch 17/20\n",
      "2809/2809 [==============================] - 0s 64us/step - loss: 0.0406 - acc: 0.7042 - auc: 0.8373 - f1: 0.7042 - val_loss: 0.0422 - val_acc: 0.8474 - val_auc: 0.8115 - val_f1: 0.8474\n",
      "Epoch 18/20\n",
      "2809/2809 [==============================] - 0s 67us/step - loss: 0.0385 - acc: 0.8156 - auc: 0.8556 - f1: 0.8156 - val_loss: 0.0413 - val_acc: 0.7475 - val_auc: 0.8088 - val_f1: 0.7475\n",
      "Epoch 19/20\n",
      "2809/2809 [==============================] - 0s 63us/step - loss: 0.0396 - acc: 0.7754 - auc: 0.8321 - f1: 0.7754 - val_loss: 0.0414 - val_acc: 0.7190 - val_auc: 0.8122 - val_f1: 0.7190\n",
      "Epoch 20/20\n",
      "2809/2809 [==============================] - 0s 68us/step - loss: 0.0402 - acc: 0.6985 - auc: 0.8455 - f1: 0.6985 - val_loss: 0.0418 - val_acc: 0.8117 - val_auc: 0.8073 - val_f1: 0.8117\n",
      "[[ 19  10]\n",
      " [122 550]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.13      0.66      0.22        29\n",
      "           1       0.98      0.82      0.89       672\n",
      "\n",
      "    accuracy                           0.81       701\n",
      "   macro avg       0.56      0.74      0.56       701\n",
      "weighted avg       0.95      0.81      0.87       701\n",
      "\n",
      "time (in seconds) 38.337827920913696\n",
      "... DONE!\n",
      "... DONE!\n",
      "window_size: 4\n",
      "generating tensor...\n",
      "...DONE!\n",
      "tensor dimensions:\n",
      "tensor input X: (3370, 8, 4)\n",
      "tensor input y: (3370, 2)\n",
      "proportion of y labels: (array([0, 1]), array([ 145, 3225]))\n",
      "Splitting dataset into Train and Test sets...\n",
      "Tensor X train: (2359, 4, 8)\n",
      "Tensor y train: (2359, 2)\n",
      "Tensor X test: (1011, 4, 8)\n",
      "Tensor y test: (1011, 2)\n",
      "computing weights...\n",
      "... DONE!\n",
      "setting stratified k-fold...\n",
      "number of k: 5\n",
      "... DONE!\n",
      "Executing algorithm...\n",
      "number of epochs: 20\n",
      "number of batch: 512\n",
      "Training on fold 1/5...\n",
      "model dim:  (4, 8) 2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_31 (GRU)                 (None, 4, 256)            203520    \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 4, 256)            0         \n",
      "_________________________________________________________________\n",
      "gru_32 (GRU)                 (None, 4, 128)            147840    \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 4, 128)            0         \n",
      "_________________________________________________________________\n",
      "gru_33 (GRU)                 (None, 64)                37056     \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 388,546\n",
      "Trainable params: 388,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1887 samples, validate on 472 samples\n",
      "Epoch 1/20\n",
      "1887/1887 [==============================] - 3s 2ms/step - loss: 0.0555 - acc: 0.5718 - auc: 0.6816 - f1: 0.5739 - val_loss: 0.0490 - val_acc: 0.7564 - val_auc: 0.7789 - val_f1: 0.7564\n",
      "Epoch 2/20\n",
      "1887/1887 [==============================] - 0s 68us/step - loss: 0.0486 - acc: 0.7901 - auc: 0.7662 - f1: 0.7901 - val_loss: 0.0513 - val_acc: 0.7945 - val_auc: 0.7742 - val_f1: 0.7945\n",
      "Epoch 3/20\n",
      "1887/1887 [==============================] - 0s 69us/step - loss: 0.0468 - acc: 0.8092 - auc: 0.8053 - f1: 0.8092 - val_loss: 0.0484 - val_acc: 0.7691 - val_auc: 0.7871 - val_f1: 0.7691\n",
      "Epoch 4/20\n",
      "1887/1887 [==============================] - 0s 70us/step - loss: 0.0437 - acc: 0.7557 - auc: 0.8303 - f1: 0.7557 - val_loss: 0.0469 - val_acc: 0.7097 - val_auc: 0.8065 - val_f1: 0.7097\n",
      "Epoch 5/20\n",
      "1887/1887 [==============================] - 0s 73us/step - loss: 0.0446 - acc: 0.7271 - auc: 0.8241 - f1: 0.7271 - val_loss: 0.0461 - val_acc: 0.7733 - val_auc: 0.8014 - val_f1: 0.7733\n",
      "Epoch 6/20\n",
      "1887/1887 [==============================] - 0s 72us/step - loss: 0.0415 - acc: 0.7896 - auc: 0.8488 - f1: 0.7896 - val_loss: 0.0461 - val_acc: 0.7924 - val_auc: 0.8055 - val_f1: 0.7924\n",
      "Epoch 7/20\n",
      "1887/1887 [==============================] - 0s 73us/step - loss: 0.0414 - acc: 0.8066 - auc: 0.8574 - f1: 0.8066 - val_loss: 0.0462 - val_acc: 0.7585 - val_auc: 0.8126 - val_f1: 0.7585\n",
      "Epoch 8/20\n",
      "1887/1887 [==============================] - 0s 75us/step - loss: 0.0419 - acc: 0.7552 - auc: 0.8476 - f1: 0.7552 - val_loss: 0.0459 - val_acc: 0.7352 - val_auc: 0.8171 - val_f1: 0.7352\n",
      "Epoch 9/20\n",
      "1887/1887 [==============================] - 0s 74us/step - loss: 0.0409 - acc: 0.7361 - auc: 0.8574 - f1: 0.7361 - val_loss: 0.0440 - val_acc: 0.7500 - val_auc: 0.8257 - val_f1: 0.7500\n",
      "Epoch 10/20\n",
      "1887/1887 [==============================] - 0s 76us/step - loss: 0.0419 - acc: 0.7954 - auc: 0.8383 - f1: 0.7954 - val_loss: 0.0444 - val_acc: 0.7775 - val_auc: 0.8221 - val_f1: 0.7775\n",
      "Epoch 11/20\n",
      "1887/1887 [==============================] - 0s 76us/step - loss: 0.0410 - acc: 0.7774 - auc: 0.8407 - f1: 0.7774 - val_loss: 0.0448 - val_acc: 0.7331 - val_auc: 0.8184 - val_f1: 0.7331\n",
      "Epoch 12/20\n",
      "1887/1887 [==============================] - 0s 76us/step - loss: 0.0409 - acc: 0.7069 - auc: 0.8513 - f1: 0.7069 - val_loss: 0.0437 - val_acc: 0.7627 - val_auc: 0.8242 - val_f1: 0.7627\n",
      "Epoch 13/20\n",
      "1887/1887 [==============================] - 0s 77us/step - loss: 0.0417 - acc: 0.8119 - auc: 0.8603 - f1: 0.8119 - val_loss: 0.0451 - val_acc: 0.8199 - val_auc: 0.8301 - val_f1: 0.8199\n",
      "Epoch 14/20\n",
      "1887/1887 [==============================] - 0s 73us/step - loss: 0.0410 - acc: 0.7843 - auc: 0.8588 - f1: 0.7843 - val_loss: 0.0453 - val_acc: 0.7076 - val_auc: 0.8291 - val_f1: 0.7076\n",
      "Epoch 15/20\n",
      "1887/1887 [==============================] - 0s 75us/step - loss: 0.0398 - acc: 0.7621 - auc: 0.8685 - f1: 0.7621 - val_loss: 0.0441 - val_acc: 0.8051 - val_auc: 0.8326 - val_f1: 0.8051\n",
      "Epoch 16/20\n",
      "1887/1887 [==============================] - 0s 75us/step - loss: 0.0404 - acc: 0.7923 - auc: 0.8607 - f1: 0.7923 - val_loss: 0.0424 - val_acc: 0.7521 - val_auc: 0.8374 - val_f1: 0.7521\n",
      "Epoch 17/20\n",
      "1887/1887 [==============================] - 0s 77us/step - loss: 0.0371 - acc: 0.7838 - auc: 0.8788 - f1: 0.7838 - val_loss: 0.0433 - val_acc: 0.7945 - val_auc: 0.8389 - val_f1: 0.7945\n",
      "Epoch 18/20\n",
      "1887/1887 [==============================] - 0s 77us/step - loss: 0.0387 - acc: 0.7896 - auc: 0.8684 - f1: 0.7896 - val_loss: 0.0430 - val_acc: 0.7288 - val_auc: 0.8361 - val_f1: 0.7288\n",
      "Epoch 19/20\n",
      "1887/1887 [==============================] - 0s 77us/step - loss: 0.0390 - acc: 0.7541 - auc: 0.8693 - f1: 0.7541 - val_loss: 0.0431 - val_acc: 0.8008 - val_auc: 0.8406 - val_f1: 0.8008\n",
      "Epoch 20/20\n",
      "1887/1887 [==============================] - 0s 76us/step - loss: 0.0376 - acc: 0.8066 - auc: 0.8883 - f1: 0.8066 - val_loss: 0.0426 - val_acc: 0.7987 - val_auc: 0.8411 - val_f1: 0.7987\n",
      "[[ 15   6]\n",
      " [ 89 362]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.14      0.71      0.24        21\n",
      "           1       0.98      0.80      0.88       451\n",
      "\n",
      "    accuracy                           0.80       472\n",
      "   macro avg       0.56      0.76      0.56       472\n",
      "weighted avg       0.95      0.80      0.86       472\n",
      "\n",
      "Training on fold 2/5...\n",
      "model dim:  (4, 8) 2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_34 (GRU)                 (None, 4, 256)            203520    \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 4, 256)            0         \n",
      "_________________________________________________________________\n",
      "gru_35 (GRU)                 (None, 4, 128)            147840    \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 4, 128)            0         \n",
      "_________________________________________________________________\n",
      "gru_36 (GRU)                 (None, 64)                37056     \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 388,546\n",
      "Trainable params: 388,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1887 samples, validate on 472 samples\n",
      "Epoch 1/20\n",
      "1887/1887 [==============================] - 3s 2ms/step - loss: 0.0585 - acc: 0.4165 - auc: 0.6306 - f1: 0.4178 - val_loss: 0.0417 - val_acc: 0.8835 - val_auc: 0.8602 - val_f1: 0.8835\n",
      "Epoch 2/20\n",
      "1887/1887 [==============================] - 0s 69us/step - loss: 0.0535 - acc: 0.8394 - auc: 0.7408 - f1: 0.8394 - val_loss: 0.0386 - val_acc: 0.7691 - val_auc: 0.8829 - val_f1: 0.7691\n",
      "Epoch 3/20\n",
      "1887/1887 [==============================] - 0s 73us/step - loss: 0.0494 - acc: 0.7048 - auc: 0.7818 - f1: 0.7048 - val_loss: 0.0406 - val_acc: 0.7246 - val_auc: 0.8834 - val_f1: 0.7246\n",
      "Epoch 4/20\n",
      "1887/1887 [==============================] - 0s 75us/step - loss: 0.0469 - acc: 0.7287 - auc: 0.7979 - f1: 0.7287 - val_loss: 0.0391 - val_acc: 0.8178 - val_auc: 0.8787 - val_f1: 0.8178\n",
      "Epoch 5/20\n",
      "1887/1887 [==============================] - 0s 76us/step - loss: 0.0482 - acc: 0.7939 - auc: 0.7878 - f1: 0.7939 - val_loss: 0.0392 - val_acc: 0.7648 - val_auc: 0.8820 - val_f1: 0.7648\n",
      "Epoch 6/20\n",
      "1887/1887 [==============================] - 0s 74us/step - loss: 0.0471 - acc: 0.7266 - auc: 0.7911 - f1: 0.7266 - val_loss: 0.0405 - val_acc: 0.6928 - val_auc: 0.8818 - val_f1: 0.6928\n",
      "Epoch 7/20\n",
      "1887/1887 [==============================] - 0s 75us/step - loss: 0.0462 - acc: 0.6789 - auc: 0.8136 - f1: 0.6789 - val_loss: 0.0379 - val_acc: 0.7458 - val_auc: 0.8817 - val_f1: 0.7458\n",
      "Epoch 8/20\n",
      "1887/1887 [==============================] - 0s 79us/step - loss: 0.0447 - acc: 0.7785 - auc: 0.8251 - f1: 0.7785 - val_loss: 0.0369 - val_acc: 0.7797 - val_auc: 0.8841 - val_f1: 0.7797\n",
      "Epoch 9/20\n",
      "1887/1887 [==============================] - 0s 77us/step - loss: 0.0451 - acc: 0.7324 - auc: 0.8231 - f1: 0.7324 - val_loss: 0.0389 - val_acc: 0.7055 - val_auc: 0.8859 - val_f1: 0.7055\n",
      "Epoch 10/20\n",
      "1887/1887 [==============================] - 0s 76us/step - loss: 0.0432 - acc: 0.7128 - auc: 0.8420 - f1: 0.7128 - val_loss: 0.0376 - val_acc: 0.7754 - val_auc: 0.8850 - val_f1: 0.7754\n",
      "Epoch 11/20\n",
      "1887/1887 [==============================] - 0s 77us/step - loss: 0.0437 - acc: 0.7827 - auc: 0.8254 - f1: 0.7827 - val_loss: 0.0375 - val_acc: 0.7818 - val_auc: 0.8841 - val_f1: 0.7818\n",
      "Epoch 12/20\n",
      "1887/1887 [==============================] - 0s 78us/step - loss: 0.0429 - acc: 0.7557 - auc: 0.8316 - f1: 0.7557 - val_loss: 0.0384 - val_acc: 0.7161 - val_auc: 0.8796 - val_f1: 0.7161\n",
      "Epoch 13/20\n",
      "1887/1887 [==============================] - 0s 77us/step - loss: 0.0428 - acc: 0.7244 - auc: 0.8326 - f1: 0.7244 - val_loss: 0.0376 - val_acc: 0.7669 - val_auc: 0.8822 - val_f1: 0.7669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20\n",
      "1887/1887 [==============================] - 0s 78us/step - loss: 0.0406 - acc: 0.7700 - auc: 0.8514 - f1: 0.7700 - val_loss: 0.0375 - val_acc: 0.7881 - val_auc: 0.8803 - val_f1: 0.7881\n",
      "Epoch 15/20\n",
      "1887/1887 [==============================] - 0s 75us/step - loss: 0.0414 - acc: 0.7785 - auc: 0.8508 - f1: 0.7785 - val_loss: 0.0386 - val_acc: 0.7352 - val_auc: 0.8732 - val_f1: 0.7352\n",
      "Epoch 16/20\n",
      "1887/1887 [==============================] - 0s 76us/step - loss: 0.0419 - acc: 0.7043 - auc: 0.8465 - f1: 0.7043 - val_loss: 0.0406 - val_acc: 0.7585 - val_auc: 0.8502 - val_f1: 0.7585\n",
      "Epoch 17/20\n",
      "1887/1887 [==============================] - 0s 80us/step - loss: 0.0431 - acc: 0.8389 - auc: 0.8504 - f1: 0.8389 - val_loss: 0.0406 - val_acc: 0.7712 - val_auc: 0.8501 - val_f1: 0.7712\n",
      "Epoch 18/20\n",
      "1887/1887 [==============================] - 0s 79us/step - loss: 0.0409 - acc: 0.6847 - auc: 0.8624 - f1: 0.6847 - val_loss: 0.0401 - val_acc: 0.6631 - val_auc: 0.8704 - val_f1: 0.6631\n",
      "Epoch 19/20\n",
      "1887/1887 [==============================] - 0s 78us/step - loss: 0.0408 - acc: 0.7700 - auc: 0.8603 - f1: 0.7700 - val_loss: 0.0389 - val_acc: 0.8432 - val_auc: 0.8784 - val_f1: 0.8432\n",
      "Epoch 20/20\n",
      "1887/1887 [==============================] - 0s 79us/step - loss: 0.0402 - acc: 0.8161 - auc: 0.8687 - f1: 0.8161 - val_loss: 0.0389 - val_acc: 0.7161 - val_auc: 0.8642 - val_f1: 0.7161\n",
      "[[ 19   2]\n",
      " [132 319]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.13      0.90      0.22        21\n",
      "           1       0.99      0.71      0.83       451\n",
      "\n",
      "    accuracy                           0.72       472\n",
      "   macro avg       0.56      0.81      0.52       472\n",
      "weighted avg       0.96      0.72      0.80       472\n",
      "\n",
      "Training on fold 3/5...\n",
      "model dim:  (4, 8) 2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_37 (GRU)                 (None, 4, 256)            203520    \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 4, 256)            0         \n",
      "_________________________________________________________________\n",
      "gru_38 (GRU)                 (None, 4, 128)            147840    \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 4, 128)            0         \n",
      "_________________________________________________________________\n",
      "gru_39 (GRU)                 (None, 64)                37056     \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 388,546\n",
      "Trainable params: 388,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1887 samples, validate on 472 samples\n",
      "Epoch 1/20\n",
      "1887/1887 [==============================] - 3s 2ms/step - loss: 0.0665 - acc: 0.6349 - auc: 0.5404 - f1: 0.6353 - val_loss: 0.0497 - val_acc: 0.6822 - val_auc: 0.8240 - val_f1: 0.6822\n",
      "Epoch 2/20\n",
      "1887/1887 [==============================] - 0s 68us/step - loss: 0.0496 - acc: 0.6953 - auc: 0.7950 - f1: 0.6953 - val_loss: 0.0502 - val_acc: 0.7669 - val_auc: 0.8076 - val_f1: 0.7669\n",
      "Epoch 3/20\n",
      "1887/1887 [==============================] - 0s 71us/step - loss: 0.0475 - acc: 0.7795 - auc: 0.8102 - f1: 0.7795 - val_loss: 0.0497 - val_acc: 0.7818 - val_auc: 0.7977 - val_f1: 0.7818\n",
      "Epoch 4/20\n",
      "1887/1887 [==============================] - 0s 72us/step - loss: 0.0460 - acc: 0.7976 - auc: 0.8046 - f1: 0.7976 - val_loss: 0.0479 - val_acc: 0.7881 - val_auc: 0.8029 - val_f1: 0.7881\n",
      "Epoch 5/20\n",
      "1887/1887 [==============================] - 0s 75us/step - loss: 0.0443 - acc: 0.7997 - auc: 0.8152 - f1: 0.7997 - val_loss: 0.0456 - val_acc: 0.7564 - val_auc: 0.8222 - val_f1: 0.7564\n",
      "Epoch 6/20\n",
      "1887/1887 [==============================] - 0s 75us/step - loss: 0.0447 - acc: 0.7509 - auc: 0.8101 - f1: 0.7509 - val_loss: 0.0445 - val_acc: 0.7542 - val_auc: 0.8364 - val_f1: 0.7542\n",
      "Epoch 7/20\n",
      "1887/1887 [==============================] - 0s 77us/step - loss: 0.0444 - acc: 0.7870 - auc: 0.8247 - f1: 0.7870 - val_loss: 0.0446 - val_acc: 0.7839 - val_auc: 0.8344 - val_f1: 0.7839\n",
      "Epoch 8/20\n",
      "1887/1887 [==============================] - 0s 76us/step - loss: 0.0416 - acc: 0.7949 - auc: 0.8423 - f1: 0.7949 - val_loss: 0.0447 - val_acc: 0.7818 - val_auc: 0.8338 - val_f1: 0.7818\n",
      "Epoch 9/20\n",
      "1887/1887 [==============================] - 0s 75us/step - loss: 0.0438 - acc: 0.7652 - auc: 0.8241 - f1: 0.7652 - val_loss: 0.0454 - val_acc: 0.7839 - val_auc: 0.8263 - val_f1: 0.7839\n",
      "Epoch 10/20\n",
      "1887/1887 [==============================] - 0s 77us/step - loss: 0.0419 - acc: 0.7965 - auc: 0.8360 - f1: 0.7965 - val_loss: 0.0475 - val_acc: 0.8136 - val_auc: 0.8156 - val_f1: 0.8136\n",
      "Epoch 11/20\n",
      "1887/1887 [==============================] - 0s 76us/step - loss: 0.0435 - acc: 0.8124 - auc: 0.8269 - f1: 0.8124 - val_loss: 0.0455 - val_acc: 0.7733 - val_auc: 0.8152 - val_f1: 0.7733\n",
      "Epoch 12/20\n",
      "1887/1887 [==============================] - 0s 79us/step - loss: 0.0423 - acc: 0.7647 - auc: 0.8438 - f1: 0.7647 - val_loss: 0.0455 - val_acc: 0.7669 - val_auc: 0.8153 - val_f1: 0.7669\n",
      "Epoch 13/20\n",
      "1887/1887 [==============================] - 0s 76us/step - loss: 0.0405 - acc: 0.8082 - auc: 0.8588 - f1: 0.8082 - val_loss: 0.0473 - val_acc: 0.8114 - val_auc: 0.8055 - val_f1: 0.8114\n",
      "Epoch 14/20\n",
      "1887/1887 [==============================] - 0s 77us/step - loss: 0.0402 - acc: 0.7917 - auc: 0.8658 - f1: 0.7917 - val_loss: 0.0468 - val_acc: 0.7945 - val_auc: 0.8016 - val_f1: 0.7945\n",
      "Epoch 15/20\n",
      "1887/1887 [==============================] - 0s 76us/step - loss: 0.0397 - acc: 0.7880 - auc: 0.8639 - f1: 0.7880 - val_loss: 0.0462 - val_acc: 0.8114 - val_auc: 0.8118 - val_f1: 0.8114\n",
      "Epoch 16/20\n",
      "1887/1887 [==============================] - 0s 78us/step - loss: 0.0401 - acc: 0.8166 - auc: 0.8606 - f1: 0.8166 - val_loss: 0.0471 - val_acc: 0.7987 - val_auc: 0.8027 - val_f1: 0.7987\n",
      "Epoch 17/20\n",
      "1887/1887 [==============================] - 0s 76us/step - loss: 0.0374 - acc: 0.7907 - auc: 0.8820 - f1: 0.7907 - val_loss: 0.0487 - val_acc: 0.8051 - val_auc: 0.7962 - val_f1: 0.8051\n",
      "Epoch 18/20\n",
      "1887/1887 [==============================] - 0s 81us/step - loss: 0.0375 - acc: 0.8060 - auc: 0.8801 - f1: 0.8060 - val_loss: 0.0490 - val_acc: 0.8136 - val_auc: 0.8015 - val_f1: 0.8136\n",
      "Epoch 19/20\n",
      "1887/1887 [==============================] - 0s 78us/step - loss: 0.0396 - acc: 0.8373 - auc: 0.8853 - f1: 0.8373 - val_loss: 0.0490 - val_acc: 0.7097 - val_auc: 0.7993 - val_f1: 0.7097\n",
      "Epoch 20/20\n",
      "1887/1887 [==============================] - 0s 78us/step - loss: 0.0393 - acc: 0.7133 - auc: 0.8779 - f1: 0.7133 - val_loss: 0.0538 - val_acc: 0.8326 - val_auc: 0.7799 - val_f1: 0.8326\n",
      "[[ 12   9]\n",
      " [ 70 381]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.15      0.57      0.23        21\n",
      "           1       0.98      0.84      0.91       451\n",
      "\n",
      "    accuracy                           0.83       472\n",
      "   macro avg       0.56      0.71      0.57       472\n",
      "weighted avg       0.94      0.83      0.88       472\n",
      "\n",
      "Training on fold 4/5...\n",
      "model dim:  (4, 8) 2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_40 (GRU)                 (None, 4, 256)            203520    \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 4, 256)            0         \n",
      "_________________________________________________________________\n",
      "gru_41 (GRU)                 (None, 4, 128)            147840    \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 4, 128)            0         \n",
      "_________________________________________________________________\n",
      "gru_42 (GRU)                 (None, 64)                37056     \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 388,546\n",
      "Trainable params: 388,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1887 samples, validate on 472 samples\n",
      "Epoch 1/20\n",
      "1887/1887 [==============================] - 4s 2ms/step - loss: 0.0526 - acc: 0.5898 - auc: 0.7436 - f1: 0.5918 - val_loss: 0.0415 - val_acc: 0.8369 - val_auc: 0.8453 - val_f1: 0.8369\n",
      "Epoch 2/20\n",
      "1887/1887 [==============================] - 0s 72us/step - loss: 0.0506 - acc: 0.7997 - auc: 0.7803 - f1: 0.7997 - val_loss: 0.0481 - val_acc: 0.6843 - val_auc: 0.8627 - val_f1: 0.6843\n",
      "Epoch 3/20\n",
      "1887/1887 [==============================] - 0s 70us/step - loss: 0.0490 - acc: 0.6953 - auc: 0.8105 - f1: 0.6953 - val_loss: 0.0401 - val_acc: 0.7691 - val_auc: 0.8616 - val_f1: 0.7691\n",
      "Epoch 4/20\n",
      "1887/1887 [==============================] - 0s 72us/step - loss: 0.0473 - acc: 0.7769 - auc: 0.7867 - f1: 0.7769 - val_loss: 0.0408 - val_acc: 0.7648 - val_auc: 0.8597 - val_f1: 0.7648\n",
      "Epoch 5/20\n",
      "1887/1887 [==============================] - 0s 73us/step - loss: 0.0456 - acc: 0.7679 - auc: 0.8082 - f1: 0.7679 - val_loss: 0.0418 - val_acc: 0.7119 - val_auc: 0.8610 - val_f1: 0.7119\n",
      "Epoch 6/20\n",
      "1887/1887 [==============================] - 0s 73us/step - loss: 0.0454 - acc: 0.6852 - auc: 0.8214 - f1: 0.6852 - val_loss: 0.0409 - val_acc: 0.7246 - val_auc: 0.8637 - val_f1: 0.7246\n",
      "Epoch 7/20\n",
      "1887/1887 [==============================] - 0s 75us/step - loss: 0.0445 - acc: 0.7705 - auc: 0.8184 - f1: 0.7705 - val_loss: 0.0394 - val_acc: 0.7839 - val_auc: 0.8630 - val_f1: 0.7839\n",
      "Epoch 8/20\n",
      "1887/1887 [==============================] - 0s 76us/step - loss: 0.0444 - acc: 0.7917 - auc: 0.8435 - f1: 0.7917 - val_loss: 0.0406 - val_acc: 0.7331 - val_auc: 0.8651 - val_f1: 0.7331\n",
      "Epoch 9/20\n",
      "1887/1887 [==============================] - 0s 74us/step - loss: 0.0424 - acc: 0.7144 - auc: 0.8469 - f1: 0.7144 - val_loss: 0.0406 - val_acc: 0.7140 - val_auc: 0.8643 - val_f1: 0.7140\n",
      "Epoch 10/20\n",
      "1887/1887 [==============================] - 0s 75us/step - loss: 0.0432 - acc: 0.7615 - auc: 0.8495 - f1: 0.7615 - val_loss: 0.0402 - val_acc: 0.7903 - val_auc: 0.8561 - val_f1: 0.7903\n",
      "Epoch 11/20\n",
      "1887/1887 [==============================] - 0s 76us/step - loss: 0.0433 - acc: 0.7562 - auc: 0.8293 - f1: 0.7562 - val_loss: 0.0416 - val_acc: 0.7013 - val_auc: 0.8524 - val_f1: 0.7013\n",
      "Epoch 12/20\n",
      "1887/1887 [==============================] - 0s 78us/step - loss: 0.0412 - acc: 0.7154 - auc: 0.8471 - f1: 0.7154 - val_loss: 0.0412 - val_acc: 0.7754 - val_auc: 0.8475 - val_f1: 0.7754\n",
      "Epoch 13/20\n",
      "1887/1887 [==============================] - 0s 79us/step - loss: 0.0410 - acc: 0.7880 - auc: 0.8532 - f1: 0.7880 - val_loss: 0.0408 - val_acc: 0.7860 - val_auc: 0.8524 - val_f1: 0.7860\n",
      "Epoch 14/20\n",
      "1887/1887 [==============================] - 0s 77us/step - loss: 0.0415 - acc: 0.7753 - auc: 0.8539 - f1: 0.7753 - val_loss: 0.0407 - val_acc: 0.7627 - val_auc: 0.8553 - val_f1: 0.7627\n",
      "Epoch 15/20\n",
      "1887/1887 [==============================] - 0s 76us/step - loss: 0.0405 - acc: 0.7276 - auc: 0.8560 - f1: 0.7276 - val_loss: 0.0412 - val_acc: 0.7564 - val_auc: 0.8502 - val_f1: 0.7564\n",
      "Epoch 16/20\n",
      "1887/1887 [==============================] - 0s 78us/step - loss: 0.0410 - acc: 0.7944 - auc: 0.8498 - f1: 0.7944 - val_loss: 0.0407 - val_acc: 0.7797 - val_auc: 0.8529 - val_f1: 0.7797\n",
      "Epoch 17/20\n",
      "1887/1887 [==============================] - 0s 77us/step - loss: 0.0391 - acc: 0.7297 - auc: 0.8659 - f1: 0.7297 - val_loss: 0.0411 - val_acc: 0.7521 - val_auc: 0.8553 - val_f1: 0.7521\n",
      "Epoch 18/20\n",
      "1887/1887 [==============================] - 0s 80us/step - loss: 0.0389 - acc: 0.7875 - auc: 0.8709 - f1: 0.7875 - val_loss: 0.0414 - val_acc: 0.8242 - val_auc: 0.8527 - val_f1: 0.8242\n",
      "Epoch 19/20\n",
      "1887/1887 [==============================] - 0s 80us/step - loss: 0.0382 - acc: 0.7610 - auc: 0.8761 - f1: 0.7610 - val_loss: 0.0424 - val_acc: 0.7055 - val_auc: 0.8519 - val_f1: 0.7055\n",
      "Epoch 20/20\n",
      "1887/1887 [==============================] - 0s 78us/step - loss: 0.0394 - acc: 0.7398 - auc: 0.8610 - f1: 0.7398 - val_loss: 0.0444 - val_acc: 0.8538 - val_auc: 0.8432 - val_f1: 0.8538\n",
      "[[ 14   7]\n",
      " [ 62 389]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.67      0.29        21\n",
      "           1       0.98      0.86      0.92       451\n",
      "\n",
      "    accuracy                           0.85       472\n",
      "   macro avg       0.58      0.76      0.60       472\n",
      "weighted avg       0.95      0.85      0.89       472\n",
      "\n",
      "Training on fold 5/5...\n",
      "model dim:  (4, 8) 2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_43 (GRU)                 (None, 4, 256)            203520    \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 4, 256)            0         \n",
      "_________________________________________________________________\n",
      "gru_44 (GRU)                 (None, 4, 128)            147840    \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 4, 128)            0         \n",
      "_________________________________________________________________\n",
      "gru_45 (GRU)                 (None, 64)                37056     \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 388,546\n",
      "Trainable params: 388,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1888 samples, validate on 471 samples\n",
      "Epoch 1/20\n",
      "1888/1888 [==============================] - 4s 2ms/step - loss: 0.0597 - acc: 0.6531 - auc: 0.6583 - f1: 0.6534 - val_loss: 0.0539 - val_acc: 0.7792 - val_auc: 0.7243 - val_f1: 0.7792\n",
      "Epoch 2/20\n",
      "1888/1888 [==============================] - 0s 70us/step - loss: 0.0470 - acc: 0.7092 - auc: 0.8186 - f1: 0.7092 - val_loss: 0.0581 - val_acc: 0.7665 - val_auc: 0.7319 - val_f1: 0.7665\n",
      "Epoch 3/20\n",
      "1888/1888 [==============================] - 0s 72us/step - loss: 0.0446 - acc: 0.7950 - auc: 0.8227 - f1: 0.7950 - val_loss: 0.0643 - val_acc: 0.8365 - val_auc: 0.7085 - val_f1: 0.8365\n",
      "Epoch 4/20\n",
      "1888/1888 [==============================] - 0s 72us/step - loss: 0.0432 - acc: 0.8173 - auc: 0.8274 - f1: 0.8173 - val_loss: 0.0582 - val_acc: 0.7728 - val_auc: 0.7237 - val_f1: 0.7728\n",
      "Epoch 5/20\n",
      "1888/1888 [==============================] - 0s 76us/step - loss: 0.0426 - acc: 0.7378 - auc: 0.8429 - f1: 0.7378 - val_loss: 0.0548 - val_acc: 0.7431 - val_auc: 0.7331 - val_f1: 0.7431\n",
      "Epoch 6/20\n",
      "1888/1888 [==============================] - 0s 77us/step - loss: 0.0403 - acc: 0.7669 - auc: 0.8565 - f1: 0.7669 - val_loss: 0.0585 - val_acc: 0.8025 - val_auc: 0.7194 - val_f1: 0.8025\n",
      "Epoch 7/20\n",
      "1888/1888 [==============================] - 0s 74us/step - loss: 0.0418 - acc: 0.7940 - auc: 0.8478 - f1: 0.7940 - val_loss: 0.0566 - val_acc: 0.7601 - val_auc: 0.7281 - val_f1: 0.7601\n",
      "Epoch 8/20\n",
      "1888/1888 [==============================] - 0s 77us/step - loss: 0.0400 - acc: 0.7579 - auc: 0.8602 - f1: 0.7579 - val_loss: 0.0579 - val_acc: 0.7622 - val_auc: 0.7268 - val_f1: 0.7622\n",
      "Epoch 9/20\n",
      "1888/1888 [==============================] - 0s 75us/step - loss: 0.0398 - acc: 0.7775 - auc: 0.8553 - f1: 0.7775 - val_loss: 0.0608 - val_acc: 0.7877 - val_auc: 0.7165 - val_f1: 0.7877\n",
      "Epoch 10/20\n",
      "1888/1888 [==============================] - 0s 76us/step - loss: 0.0400 - acc: 0.7765 - auc: 0.8597 - f1: 0.7765 - val_loss: 0.0591 - val_acc: 0.7622 - val_auc: 0.7268 - val_f1: 0.7622\n",
      "Epoch 11/20\n",
      "1888/1888 [==============================] - 0s 78us/step - loss: 0.0392 - acc: 0.7696 - auc: 0.8795 - f1: 0.7696 - val_loss: 0.0593 - val_acc: 0.7792 - val_auc: 0.7303 - val_f1: 0.7792\n",
      "Epoch 12/20\n",
      "1888/1888 [==============================] - 0s 78us/step - loss: 0.0370 - acc: 0.7807 - auc: 0.8802 - f1: 0.7807 - val_loss: 0.0585 - val_acc: 0.7856 - val_auc: 0.7321 - val_f1: 0.7856\n",
      "Epoch 13/20\n",
      "1888/1888 [==============================] - 0s 77us/step - loss: 0.0380 - acc: 0.7505 - auc: 0.8744 - f1: 0.7505 - val_loss: 0.0577 - val_acc: 0.7707 - val_auc: 0.7328 - val_f1: 0.7707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20\n",
      "1888/1888 [==============================] - 0s 76us/step - loss: 0.0376 - acc: 0.7722 - auc: 0.8764 - f1: 0.7722 - val_loss: 0.0606 - val_acc: 0.8174 - val_auc: 0.7259 - val_f1: 0.8174\n",
      "Epoch 15/20\n",
      "1888/1888 [==============================] - 0s 77us/step - loss: 0.0376 - acc: 0.8157 - auc: 0.8802 - f1: 0.8157 - val_loss: 0.0588 - val_acc: 0.7877 - val_auc: 0.7335 - val_f1: 0.7877\n",
      "Epoch 16/20\n",
      "1888/1888 [==============================] - 0s 75us/step - loss: 0.0356 - acc: 0.7728 - auc: 0.8924 - f1: 0.7728 - val_loss: 0.0637 - val_acc: 0.7962 - val_auc: 0.7375 - val_f1: 0.7962\n",
      "Epoch 17/20\n",
      "1888/1888 [==============================] - 0s 80us/step - loss: 0.0374 - acc: 0.8146 - auc: 0.8946 - f1: 0.8146 - val_loss: 0.0713 - val_acc: 0.8301 - val_auc: 0.7307 - val_f1: 0.8301\n",
      "Epoch 18/20\n",
      "1888/1888 [==============================] - 0s 80us/step - loss: 0.0358 - acc: 0.8040 - auc: 0.8940 - f1: 0.8040 - val_loss: 0.0613 - val_acc: 0.7495 - val_auc: 0.7407 - val_f1: 0.7495\n",
      "Epoch 19/20\n",
      "1888/1888 [==============================] - 0s 79us/step - loss: 0.0342 - acc: 0.7760 - auc: 0.9071 - f1: 0.7760 - val_loss: 0.0660 - val_acc: 0.8386 - val_auc: 0.7331 - val_f1: 0.8386\n",
      "Epoch 20/20\n",
      "1888/1888 [==============================] - 0s 79us/step - loss: 0.0338 - acc: 0.8242 - auc: 0.9036 - f1: 0.8242 - val_loss: 0.0589 - val_acc: 0.7707 - val_auc: 0.7567 - val_f1: 0.7707\n",
      "[[ 14   7]\n",
      " [101 349]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.67      0.21        21\n",
      "           1       0.98      0.78      0.87       450\n",
      "\n",
      "    accuracy                           0.77       471\n",
      "   macro avg       0.55      0.72      0.54       471\n",
      "weighted avg       0.94      0.77      0.84       471\n",
      "\n",
      "time (in seconds) 42.675801515579224\n",
      "... DONE!\n",
      "... DONE!\n"
     ]
    }
   ],
   "source": [
    "listResults = {} \n",
    "for i in [2, 3, 4]:\n",
    "    print(\"window_size: \"+str(i))\n",
    "    listResults[i] = applyTensor(i, df)\n",
    "    print(\"... DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FROM WINDOW SIZE 2\n",
      "[[  78   16]\n",
      " [ 593 1323]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.83      0.20        94\n",
      "           1       0.99      0.69      0.81      1916\n",
      "\n",
      "    accuracy                           0.70      2010\n",
      "   macro avg       0.55      0.76      0.51      2010\n",
      "weighted avg       0.95      0.70      0.78      2010\n",
      "\n",
      "ROC_AUC: 0.7601441389419556\n",
      "F1-Score: 0.7421894116172809\n",
      "Precision: 0.8022423202466961\n",
      "Recall: 0.6905010438413581\n",
      "Accuracy: 0.7601441389419448\n",
      "Time in seconds: 29.54588747024536\n",
      "\n",
      "\n",
      "RESULTS FROM WINDOW SIZE 3\n",
      "[[  51   24]\n",
      " [ 268 1162]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.16      0.68      0.26        75\n",
      "           1       0.98      0.81      0.89      1430\n",
      "\n",
      "    accuracy                           0.81      1505\n",
      "   macro avg       0.57      0.75      0.57      1505\n",
      "weighted avg       0.94      0.81      0.86      1505\n",
      "\n",
      "ROC_AUC: 0.7462937062937034\n",
      "F1-Score: 0.7620671563483704\n",
      "Precision: 0.7174611015065436\n",
      "Recall: 0.812587412587407\n",
      "Accuracy: 0.7462937062937064\n",
      "Time in seconds: 38.337827920913696\n",
      "\n",
      "\n",
      "RESULTS FROM WINDOW SIZE 4\n",
      "[[ 32   8]\n",
      " [263 708]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.11      0.80      0.19        40\n",
      "           1       0.99      0.73      0.84       971\n",
      "\n",
      "    accuracy                           0.73      1011\n",
      "   macro avg       0.55      0.76      0.52      1011\n",
      "weighted avg       0.95      0.73      0.81      1011\n",
      "\n",
      "ROC_AUC: 0.7645726055612799\n",
      "F1-Score: 0.7559256886611204\n",
      "Precision: 0.7847483928175626\n",
      "Recall: 0.7291452111225599\n",
      "Accuracy: 0.764572605561277\n",
      "Time in seconds: 42.675801515579224\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in [2, 3, 4]:\n",
    "    print(\"RESULTS FROM WINDOW SIZE\",i)\n",
    "    lastModel = listResults[i][0]\n",
    "    history_general = listResults[i][1]\n",
    "    X_test = listResults[i][2]\n",
    "    y_test = listResults[i][3]\n",
    "    time_in_seconds = listResults[i][4]\n",
    "\n",
    "    fractions_t = 1-y_test.sum(axis=0)/len(y_test)\n",
    "    weights_t = fractions_t[y_test.argmax(axis=1)]\n",
    "\n",
    "    output = lastModel.predict_classes(X_test)\n",
    "    print(confusion_matrix(y_test.argmax(axis=1), output))\n",
    "    print(classification_report(y_test.argmax(axis=1), output))\n",
    "    evaluation_metrics(y_test.argmax(axis=1), output, weights_t)\n",
    "    print(\"Time in seconds:\", time_in_seconds)\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
